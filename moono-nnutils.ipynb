{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# -*- coding: utf-8 -*-\n", "# \n", "# # Copyright (c) 2019, NVIDIA Corporation. All rights reserved.\n", "#\n", "import os\n", "import io\n", "from io import StringIO\n", "import time\n", "import argparse\n", "import functools\n", "import errno\n", "import scipy\n", "import scipy.io\n", "import requests\n", "import zipfile\n", "import random\n", "import datetime\n", "#\n", "from functools import partial\n", "from importlib import import_module\n", "#\n", "import logging\n", "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n", "#\n", "import numpy as np\n", "from numpy import *\n", "#\n", "import math\n", "from math import floor, log2\n", "from random import random\n", "from pylab import *\n", "from IPython.core.display import display\n", "import PIL\n", "from PIL import Image\n", "PIL.Image.MAX_IMAGE_PIXELS = 933120000\n", "#\n", "import scipy.ndimage as pyimg\n", "import cv2\n", "import imageio\n", "import glob\n", "import matplotlib as mpl\n", "import matplotlib.pyplot as plt \n", "import matplotlib.image as mgimg\n", "import matplotlib.animation as anim\n", "mpl.rcParams['figure.figsize'] = (12,12)\n", "mpl.rcParams['axes.grid'] = False\n", "#\n", "import shutil\n", "import gdown\n", "#\n", "import sys\n", "#\n", "import tensorflow as tf \n", "from tensorflow.keras import initializers, regularizers, constraints\n", "from tensorflow.keras import backend as K\n", "from tensorflow.keras import layers\n", "from tensorflow.keras.layers import Layer, InputSpec\n", "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D, UpSampling2D, Conv2D\n", "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, LeakyReLU\n", "from tensorflow.keras.models import Sequential, Model\n", "from tensorflow.keras.optimizers import Adam\n", "from tensorflow.python.keras.utils import conv_utils\n", "#\n", "from tensorflow.keras.layers import Lambda\n", "from tensorflow.keras.layers import add\n", "from tensorflow.keras.layers import AveragePooling2D\n", "from tensorflow.keras.initializers import VarianceScaling\n", "from tensorflow.keras.models import clone_model\n", "from tensorflow.keras.models import model_from_json\n", "#\n", "from absl import app\n", "from absl import flags\n", "from absl import logging\n", "#\n", "tf.get_logger().setLevel('ERROR')\n", "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n", "#\n", "print(f'|===> {tf.__version__}')\n", "#\n", "if 1: # get base.py from github\n", "    cwd = os.getcwd()\n", "    base_path = os.path.join(cwd, 'base.py')\n", "    if not os.path.exists(base_path):\n", "        base_file = 'base.py'\n", "        urlfolder = 'https://raw.githubusercontent.com/sifbuilder/pylon/master/'\n", "        url = f'{urlfolder}{base_file}'\n", "        print(f\"|===> nnimg: get base file \\n \\\n", "            urlfolder: {urlfolder} \\n \\\n", "            url: {url} \\n \\\n", "            base_path: {base_path} \\n \\\n", "        \")\n", "        tofile = tf.keras.utils.get_file(f'{base_path}', origin=url, extract=True)\n", "    else:\n", "        print(f\"|===> base in cwd {cwd}\")\n", "#\n", "#\n", "#   FUNS\n", "#\n", "#\n", "# check if base.Onpyon is defined\n", "try:\n", "    var = Onpyon()\n", "except NameError:\n", "    sys.path.append('../')  # if called from eon, modules are in parallel folder\n", "    sys.path.append('./')  #  if called from dnns, modules are in folder\n", "    from base import *\n", "#\n", "onutil = Onutil()\n", "onplot = Onplot()\n", "onformat = Onformat()\n", "onfile = Onfile()\n", "onvid = Onvid()\n", "onimg = Onimg()\n", "ondata = Ondata()\n", "onset = Onset()\n", "onrecord = Onrecord()\n", "ontree = Ontree()\n", "#\n", "onvgg = Onvgg()\n", "onlllyas = Onlllyas()\n", "oncuda = Oncuda()\n", "onrosa = Onrosa()\n", "onmoono = Onmoono()\n", "#\n", "#\n", "#   CONTEXT\n", "#\n", "#\n", "#\n", "def getap():\n", "    # https://github.com/moono/stylegan2-tf-2.x/tree/master/stylegan2\n", "    cp = {\n", "        \"primecmd\": 'nnutils',    \n", "                \n", "        \"MNAME\": \"moono\",      \n", "        \"AUTHOR\": \"moono\",      \n", "        \"PROJECT\": \"stylegan2\",      \n", "        \"GITPOD\": \"stylegan2-tf-2.x\",      \n", "        \"DATASET\": \"ffhq\",        \n", "    \n", "        \"GDRIVE\": 1,            # mount gdrive: gdata, gwork    \n", "        \"TRAINDO\": 1,      \n", "        \"MAKEGIF\": 1,      \n", "        \"RUNGIF\": 0,      \n", "        \"CLEARTMP\": 0,      \n", "        \"REGET\": 0,             # get again data \n", "        \"ING\": 1,               # ckpt in gwork\n", "        \"MODITEM\": \"\",          # will look into module\n", "        \"RESETCODE\": False,\n", "        \"LOCALDATA\": False,\n", "        \"LOCALMODELS\": False,\n", "        \"LOCALLAB\": True,\n", "        \"grel_infix\": '../..',            # relative path to content \n", "        \"net_prefix\": '//enas/hdrive',     \n", "        \"gdrive_prefix\": '/content/drive/My Drive',     \n", "        \"gcloud_prefix\": '/content',     \n", "    }\n", "    local_prefix = os.path.abspath('')\n", "    try:\n", "        local_prefix = os.path.dirname(os.path.realpath(__file__)) # script dir\n", "    except:\n", "        pass\n", "    cp[\"local_prefix\"] = local_prefix\n", "    hp = {\n", "        \"verbose\": False,\n", "        \"visual\": True,\n", "    }\n", "    ap = {}\n", "    for key in cp.keys():\n", "        ap[key] = cp[key]\n", "    for key in hp.keys():\n", "        ap[key] = hp[key]\n", "    return ap\n", "#\n", "def getxp(cp):\n", "    yp={}\n", "    xp={}\n", "    for key in cp.keys():\n", "        xp[key] = cp[key]\n", "    tree = ontree.tree(cp)\n", "    for key in tree.keys():\n", "        xp[key] = tree[key]\n", "    for key in yp.keys():\n", "        xp[key] = yp[key]\n", "   \n", "    return xp\n", "#\n", "#\n", "#   NETS\n", "#\n", "#\n", "class Dense(tf.keras.layers.Layer):\n", "    def __init__(self, fmaps, gain, lrmul, **kwargs):\n", "        super(Dense, self).__init__(**kwargs)\n", "        self.fmaps = fmaps\n", "        self.gain = gain\n", "        self.lrmul = lrmul\n", "    def build(self, input_shape):\n", "        assert len(input_shape) == 2 or len(input_shape) == 4\n", "        fan_in = np.prod(input_shape[1:])\n", "        weight_shape = [fan_in, self.fmaps]\n", "        init_std, runtime_coef = onmoono.compute_runtime_coef(weight_shape, self.gain, self.lrmul)\n", "        self.runtime_coef = runtime_coef\n", "        w_init = tf.random.normal(shape=weight_shape, mean=0.0, stddev=init_std)\n", "        self.w = tf.Variable(w_init, name='w', trainable=True)\n", "    def call(self, inputs, training=None, mask=None):\n", "        weight = self.runtime_coef * self.w\n", "        x = tf.keras.layers.Flatten()(inputs)\n", "        x = tf.matmul(x, weight)\n", "        return x\n", "    def get_config(self):\n", "        config = super(Dense, self).get_config()\n", "        config.update({\n", "            'fmaps': self.fmaps,\n", "            'gain': self.gain,\n", "            'lrmul': self.lrmul,\n", "        })\n", "        return config\n", "#\n", "class Bias(tf.keras.layers.Layer):\n", "    def __init__(self, lrmul, **kwargs):\n", "        super(Bias, self).__init__(**kwargs)\n", "        self.lrmul = lrmul\n", "    def build(self, input_shape):\n", "        assert len(input_shape) == 2 or len(input_shape) == 4\n", "        self.len2 = True if len(input_shape) == 2 else False\n", "        b_init = tf.zeros(shape=(input_shape[1],), dtype=tf.dtypes.float32)\n", "        self.b = tf.Variable(b_init, name='b', trainable=True)\n", "    def call(self, inputs, training=None, mask=None):\n", "        b = self.lrmul * self.b\n", "        if self.len2:\n", "            x = inputs + b\n", "        else:\n", "            x = inputs + tf.reshape(b, [1, -1, 1, 1])\n", "        return x\n", "    def get_config(self):\n", "        config = super(Bias, self).get_config()\n", "        config.update({\n", "            'lrmul': self.lrmul,\n", "        })\n", "        return config\n", "#\n", "class LeakyReLU(tf.keras.layers.Layer):\n", "    def __init__(self, **kwargs):\n", "        super(LeakyReLU, self).__init__(**kwargs)\n", "        self.alpha = 0.2\n", "        self.gain = np.sqrt(2)\n", "        self.act = tf.keras.layers.LeakyReLU(alpha=self.alpha)\n", "    def call(self, inputs, training=None, mask=None):\n", "        x = self.act(inputs)\n", "        x *= self.gain\n", "        return x\n", "    def get_config(self):\n", "        config = super(LeakyReLU, self).get_config()\n", "        config.update({\n", "            'alpha': self.alpha,\n", "            'gain': self.gain,\n", "        })\n", "        return config\n", "#\n", "class LabelEmbedding(tf.keras.layers.Layer):\n", "    def __init__(self, embed_dim, **kwargs):\n", "        super(LabelEmbedding, self).__init__(**kwargs)\n", "        self.embed_dim = embed_dim\n", "    def build(self, input_shape):\n", "        weight_shape = [input_shape[1], self.embed_dim]\n", "        # tf 1.15 mean(0.0), std(1.0) default value of tf.initializers.random_normal()\n", "        w_init = tf.random.normal(shape=weight_shape, mean=0.0, stddev=1.0)\n", "        self.w = tf.Variable(w_init, name='w', trainable=True)\n", "    def call(self, inputs, training=None, mask=None):\n", "        x = tf.matmul(inputs, self.w)\n", "        return x\n", "    def get_config(self):\n", "        config = super(LabelEmbedding, self).get_config()\n", "        config.update({\n", "            'embed_dim': self.embed_dim,\n", "        })\n", "        return config\n", "#\n", "class Noise(tf.keras.layers.Layer):\n", "    def __init__(self, **kwargs):\n", "        super(Noise, self).__init__(**kwargs)\n", "    def build(self, input_shape):\n", "        self.noise_strength = tf.Variable(initial_value=0.0, dtype=tf.dtypes.float32, trainable=True, name='w')\n", "    def call(self, x, training=None, mask=None):\n", "        x_shape = tf.shape(x)\n", "        noise = tf.random.normal(shape=(x_shape[0], 1, x_shape[2], x_shape[3]), dtype=tf.dtypes.float32)\n", "        x += noise * self.noise_strength\n", "        return x\n", "#\n", "class MinibatchStd(tf.keras.layers.Layer):\n", "    def __init__(self, group_size, num_new_features, **kwargs):\n", "        super(MinibatchStd, self).__init__(**kwargs)\n", "        self.group_size = group_size\n", "        self.num_new_features = num_new_features\n", "    def call(self, x, training=None, mask=None):\n", "        group_size = tf.minimum(self.group_size, tf.shape(x)[0])\n", "        s = tf.shape(x)\n", "        y = tf.reshape(x, [group_size, -1, self.num_new_features, s[1] // self.num_new_features, s[2], s[3]])\n", "        y = tf.cast(y, tf.float32)\n", "        y -= tf.reduce_mean(y, axis=0, keepdims=True)\n", "        y = tf.reduce_mean(tf.square(y), axis=0)\n", "        y = tf.sqrt(y + 1e-8)\n", "        y = tf.reduce_mean(y, axis=[2, 3, 4], keepdims=True)\n", "        y = tf.reduce_mean(y, axis=[2])\n", "        y = tf.cast(y, x.dtype)\n", "        y = tf.tile(y, [group_size, 1, s[2], s[3]])\n", "        return tf.concat([x, y], axis=1)\n", "#\n", "class FusedModConv(tf.keras.layers.Layer):\n", "    def __init__(self, fmaps, kernel, gain, lrmul, style_fmaps, demodulate, up, down, resample_kernel, **kwargs):\n", "        super(FusedModConv, self).__init__(**kwargs)\n", "        assert not (up and down)\n", "        self.fmaps = fmaps\n", "        self.kernel = kernel\n", "        self.gain = gain\n", "        self.lrmul = lrmul\n", "        self.style_fmaps = style_fmaps\n", "        self.demodulate = demodulate\n", "        self.up = up\n", "        self.down = down\n", "        self.factor = 2\n", "        if resample_kernel is None:\n", "            resample_kernel = [1] * self.factor\n", "        self.k = onmoono.setup_resample_kernel(k=resample_kernel)\n", "        self.mod_dense = Dense(self.style_fmaps, gain=1.0, lrmul=1.0, name='mod_dense')\n", "        self.mod_bias = Bias(lrmul=1.0, name='mod_bias')\n", "    def build(self, input_shape):\n", "        x_shape, w_shape = input_shape[0], input_shape[1]\n", "        weight_shape = [self.kernel, self.kernel, x_shape[1], self.fmaps]\n", "        init_std, runtime_coef = onmoono.compute_runtime_coef(weight_shape, self.gain, self.lrmul)\n", "        self.runtime_coef = runtime_coef\n\n", "        # [kkIO]\n", "        w_init = tf.random.normal(shape=weight_shape, mean=0.0, stddev=init_std)\n", "        self.w = tf.Variable(w_init, name='w', trainable=True)\n", "    def scale_conv_weights(self, w):\n", "        # convolution kernel weights for fused conv\n", "        weight = self.runtime_coef * self.w     # [kkIO]\n", "        weight = weight[np.newaxis]             # [BkkIO]\n\n", "        # modulation\n", "        style = self.mod_dense(w)                                   # [BI]\n", "        style = self.mod_bias(style) + 1.0                          # [BI]\n", "        weight *= style[:, np.newaxis, np.newaxis, :, np.newaxis]   # [BkkIO]\n\n", "        # demodulation\n", "        if self.demodulate:\n", "            d = tf.math.rsqrt(tf.reduce_sum(tf.square(weight), axis=[1, 2, 3]) + 1e-8)  # [BO]\n", "            weight *= d[:, np.newaxis, np.newaxis, np.newaxis, :]                       # [BkkIO]\n\n", "        # weight: reshape, prepare for fused operation\n", "        new_weight_shape = [tf.shape(weight)[1], tf.shape(weight)[2], tf.shape(weight)[3], -1]      # [kkI(BO)]\n", "        weight = tf.transpose(weight, [1, 2, 3, 0, 4])                                              # [kkIBO]\n", "        weight = tf.reshape(weight, shape=new_weight_shape)                                         # [kkI(BO)]\n", "        return weight\n", "    def call(self, inputs, training=None, mask=None):\n", "        x, w = inputs\n", "        height, width = tf.shape(x)[2], tf.shape(x)[3]\n\n", "        # prepare convolution kernel weights\n", "        weight = self.scale_conv_weights(w)\n\n", "        # prepare inputs: reshape minibatch to convolution groups\n", "        x = tf.reshape(x, [1, -1, height, width])\n", "        if self.up:\n", "            x = onmoono.upsample_conv_2d(x, self.k, weight, self.factor, self.gain)\n", "        elif self.down:\n", "            x = onmoono.conv_downsample_2d(x, self.k, weight, self.factor, self.gain)\n", "        else:\n", "            x = tf.nn.conv2d(x, weight, data_format='NCHW', strides=[1, 1, 1, 1], padding='SAME')\n\n", "        # x: reshape back\n", "        x = tf.reshape(x, [-1, self.fmaps, tf.shape(x)[2], tf.shape(x)[3]])\n", "        return x\n", "    def get_config(self):\n", "        config = super(FusedModConv, self).get_config()\n", "        config.update({\n", "            'fmaps': self.fmaps,\n", "            'kernel': self.kernel,\n", "            'gain': self.gain,\n", "            'lrmul': self.lrmul,\n", "            'style_fmaps': self.style_fmaps,\n", "            'demodulate': self.demodulate,\n", "            'up': self.up,\n", "            'down': self.down,\n", "            'factor': self.factor,\n", "            'k': self.k,\n", "        })\n", "        return config\n", "#\n", "class ResizeConv2D(tf.keras.layers.Layer):\n", "    def __init__(self, fmaps, kernel, gain, lrmul, up, down, resample_kernel, **kwargs):\n", "        super(ResizeConv2D, self).__init__(**kwargs)\n", "        self.fmaps = fmaps\n", "        self.kernel = kernel\n", "        self.gain = gain\n", "        self.lrmul = lrmul\n", "        self.up = up\n", "        self.down = down\n", "        self.factor = 2\n", "        if resample_kernel is None:\n", "            resample_kernel = [1] * self.factor\n", "        self.k = onmoono.setup_resample_kernel(k=resample_kernel)\n", "    def build(self, input_shape):\n", "        assert len(input_shape) == 4\n", "        weight_shape = [self.kernel, self.kernel, input_shape[1], self.fmaps]\n", "        init_std, runtime_coef = onmoono.compute_runtime_coef(weight_shape, self.gain, self.lrmul)\n", "        self.runtime_coef = runtime_coef\n\n", "        # [kernel, kernel, fmaps_in, fmaps_out]\n", "        w_init = tf.random_normal_initializer(mean=0.0, stddev=init_std)\n", "        self.w = tf.Variable(initial_value=w_init(shape=weight_shape, dtype='float32'), trainable=True, name='w')\n", "    def call(self, inputs, training=None, mask=None):\n", "        x = inputs\n", "        weight = self.runtime_coef * self.w\n", "        if self.up:\n", "            x = onmoono.upsample_conv_2d(x, self.k, weight, self.factor, self.gain)\n", "        elif self.down:\n", "            x = onmoono.conv_downsample_2d(x, self.k, weight, self.factor, self.gain)\n", "        else:\n", "            x = tf.nn.conv2d(x, weight, data_format='NCHW', strides=[1, 1, 1, 1], padding='SAME')\n", "        return x\n", "    def get_config(self):\n", "        config = super(ResizeConv2D, self).get_config()\n", "        config.update({\n", "            'fmaps': self.fmaps,\n", "            'kernel': self.kernel,\n", "            'gain': self.gain,\n", "            'lrmul': self.lrmul,\n", "            'up': self.up,\n", "            'down': self.down,\n", "            'factor': self.factor,\n", "            'k': self.k,\n", "        })\n", "        return config\n", "#\n", "#\n", "#\n", "#    -> https://github.com/moono/stylegan2-tf-2.x/tree/master/stylegan2/losses.py <-\n", "#\n", "#\n", "#\n", "def g_logistic_non_saturating(generator, discriminator, z, labels):\n", "    # forward pass\n", "    fake_images = generator([z, labels], training=True)\n", "    fake_scores = discriminator([fake_images, labels], training=True)\n\n", "    # gan loss\n", "    g_loss = tf.math.softplus(-fake_scores)\n", "    # g_loss = tf.reduce_mean(g_loss)\n", "    return g_loss\n", "#\n", "def pl_reg(fake_images, w_broadcasted, pl_mean, pl_decay=0.01):\n", "    h, w = fake_images.shape[2], fake_images.shape[3]\n\n", "    # Compute |J*y|.\n", "    with tf.GradientTape() as pl_tape:\n", "        pl_tape.watch(w_broadcasted)\n", "        pl_noise = tf.random.normal(tf.shape(fake_images), mean=0.0, stddev=1.0, dtype=tf.float32) / np.sqrt(h * w)\n", "    pl_grads = pl_tape.gradient(tf.reduce_sum(fake_images * pl_noise), w_broadcasted)\n", "    pl_lengths = tf.math.sqrt(tf.reduce_mean(tf.reduce_sum(tf.math.square(pl_grads), axis=2), axis=1))\n\n", "    # Track exponential moving average of |J*y|.\n", "    pl_mean_val = pl_mean + pl_decay * (tf.reduce_mean(pl_lengths) - pl_mean)\n", "    pl_mean.assign(pl_mean_val)\n\n", "    # Calculate (|J*y|-a)^2.\n", "    pl_penalty = tf.square(pl_lengths - pl_mean)\n", "    return pl_penalty\n", "#\n", "def d_logistic(generator, discriminator, z, labels, real_images):\n", "    # forward pass\n", "    fake_images = generator([z, labels], training=True)\n", "    real_scores = discriminator([real_images, labels], training=True)\n", "    fake_scores = discriminator([fake_images, labels], training=True)\n\n", "    # gan loss\n", "    d_loss = tf.math.softplus(fake_scores)\n", "    d_loss += tf.math.softplus(-real_scores)\n", "    return d_loss\n", "#\n", "def r1_reg(discriminator, labels, real_images):\n", "    # simple GP\n", "    with tf.GradientTape() as r1_tape:\n", "        r1_tape.watch(real_images)\n", "        real_loss = tf.reduce_sum(discriminator([real_images, labels], training=True))\n", "    real_grads = r1_tape.gradient(real_loss, real_images)\n", "    r1_penalty = tf.reduce_sum(tf.math.square(real_grads), axis=[1, 2, 3])\n", "    r1_penalty = tf.expand_dims(r1_penalty, axis=1)\n", "    return r1_penalty\n", "#\n", "#\n", "#    -> https://github.com/moono/stylegan2-tf-2.x/tree/master/stylegan2/generator.py <-\n", "#\n", "#\n", "class ToRGB(tf.keras.layers.Layer):\n", "    def __init__(self, in_ch, **kwargs):\n", "        super(ToRGB, self).__init__(**kwargs)\n", "        self.in_ch = in_ch\n", "        self.conv = FusedModConv(fmaps=3, kernel=1, gain=1.0, lrmul=1.0, style_fmaps=self.in_ch,\n", "                                 demodulate=False, up=False, down=False, resample_kernel=None, name='conv')\n", "        self.apply_bias = Bias(lrmul=1.0, name='bias')\n", "    def call(self, inputs, training=None, mask=None):\n", "        x, w = inputs\n", "        assert x.shape[1] == self.in_ch\n", "        x = self.conv([x, w])\n", "        x = self.apply_bias(x)\n", "        return x\n", "    def get_config(self):\n", "        config = super(ToRGB, self).get_config()\n", "        config.update({\n", "            'in_ch': self.in_ch,\n", "        })\n", "        return config\n", "#\n", "class Mapping(tf.keras.layers.Layer):\n", "    def __init__(self, w_dim, labels_dim, n_mapping, **kwargs):\n", "        super(Mapping, self).__init__(**kwargs)\n", "        self.w_dim = w_dim\n", "        self.labels_dim = labels_dim\n", "        self.n_mapping = n_mapping\n", "        self.gain = 1.0\n", "        self.lrmul = 0.01\n", "        if self.labels_dim > 0:\n", "            self.labels_embedding = LabelEmbedding(embed_dim=self.w_dim, name='labels_embedding')\n", "        self.normalize = tf.keras.layers.Lambda(lambda x: x * tf.math.rsqrt(tf.reduce_mean(tf.square(x), axis=1, keepdims=True) + 1e-8))\n", "        self.dense_layers = list()\n", "        self.bias_layers = list()\n", "        self.act_layers = list()\n", "        for ii in range(self.n_mapping):\n", "            self.dense_layers.append(Dense(w_dim, gain=self.gain, lrmul=self.lrmul, name='dense_{:d}'.format(ii)))\n", "            self.bias_layers.append(Bias(lrmul=self.lrmul, name='bias_{:d}'.format(ii)))\n", "            self.act_layers.append(LeakyReLU(name='lrelu_{:d}'.format(ii)))\n", "    def call(self, inputs, training=None, mask=None):\n", "        latents, labels = inputs\n", "        x = latents\n\n", "        # embed label if any\n", "        if self.labels_dim > 0:\n", "            y = self.labels_embedding(labels)\n", "            x = tf.concat([x, y], axis=1)\n\n", "        # normalize inputs\n", "        x = self.normalize(x)\n\n", "        # apply mapping blocks\n", "        for dense, apply_bias, leaky_relu in zip(self.dense_layers, self.bias_layers, self.act_layers):\n", "            x = dense(x)\n", "            x = apply_bias(x)\n", "            x = leaky_relu(x)\n", "        return x\n", "    def get_config(self):\n", "        config = super(Mapping, self).get_config()\n", "        config.update({\n", "            'w_dim': self.w_dim,\n", "            'labels_dim': self.labels_dim,\n", "            'n_mapping': self.n_mapping,\n", "            'gain': self.gain,\n", "            'lrmul': self.lrmul,\n", "        })\n", "        return config\n", "#\n", "class SynthesisConstBlock(tf.keras.layers.Layer):\n", "    def __init__(self, fmaps, res, **kwargs):\n", "        super(SynthesisConstBlock, self).__init__(**kwargs)\n", "        assert res == 4\n", "        self.res = res\n", "        self.fmaps = fmaps\n", "        self.gain = 1.0\n", "        self.lrmul = 1.0\n\n", "        # conv block\n", "        self.conv = FusedModConv(fmaps=self.fmaps, kernel=3, gain=self.gain, lrmul=self.lrmul, style_fmaps=self.fmaps,\n", "                                 demodulate=True, up=False, down=False, resample_kernel=[1, 3, 3, 1], name='conv')\n", "        self.apply_noise = Noise(name='noise')\n", "        self.apply_bias = Bias(lrmul=self.lrmul, name='bias')\n", "        self.leaky_relu = LeakyReLU(name='lrelu')\n", "    def build(self, input_shape):\n", "        # starting const variable\n", "        # tf 1.15 mean(0.0), std(1.0) default value of tf.initializers.random_normal()\n", "        const_init = tf.random.normal(shape=(1, self.fmaps, self.res, self.res), mean=0.0, stddev=1.0)\n", "        self.const = tf.Variable(const_init, name='const', trainable=True)\n", "    def call(self, inputs, training=None, mask=None):\n", "        w0 = inputs\n", "        batch_size = tf.shape(w0)[0]\n\n", "        # const block\n", "        x = tf.tile(self.const, [batch_size, 1, 1, 1])\n\n", "        # conv block\n", "        x = self.conv([x, w0])\n", "        x = self.apply_noise(x)\n", "        x = self.apply_bias(x)\n", "        x = self.leaky_relu(x)\n", "        return x\n", "    def get_config(self):\n", "        config = super(SynthesisConstBlock, self).get_config()\n", "        config.update({\n", "            'res': self.res,\n", "            'fmaps': self.fmaps,\n", "            'gain': self.gain,\n", "            'lrmul': self.lrmul,\n", "        })\n", "        return config\n", "#\n", "class SynthesisBlock(tf.keras.layers.Layer):\n", "    def __init__(self, in_ch, fmaps, res, **kwargs):\n", "        super(SynthesisBlock, self).__init__(**kwargs)\n", "        self.in_ch = in_ch\n", "        self.fmaps = fmaps\n", "        self.res = res\n", "        self.gain = 1.0\n", "        self.lrmul = 1.0\n\n", "        # conv0 up\n", "        self.conv_0 = FusedModConv(fmaps=self.fmaps, kernel=3, gain=self.gain, lrmul=self.lrmul, style_fmaps=self.in_ch,\n", "                                   demodulate=True, up=True, down=False, resample_kernel=[1, 3, 3, 1], name='conv_0')\n", "        self.apply_noise_0 = Noise(name='noise_0')\n", "        self.apply_bias_0 = Bias(lrmul=self.lrmul, name='bias_0')\n", "        self.leaky_relu_0 = LeakyReLU(name='lrelu_0')\n\n", "        # conv block\n", "        self.conv_1 = FusedModConv(fmaps=self.fmaps, kernel=3, gain=self.gain, lrmul=self.lrmul, style_fmaps=self.fmaps,\n", "                                   demodulate=True, up=False, down=False, resample_kernel=[1, 3, 3, 1], name='conv_1')\n", "        self.apply_noise_1 = Noise(name='noise_1')\n", "        self.apply_bias_1 = Bias(lrmul=self.lrmul, name='bias_1')\n", "        self.leaky_relu_1 = LeakyReLU(name='lrelu_1')\n", "    def call(self, inputs, training=None, mask=None):\n", "        x, w0, w1 = inputs\n\n", "        # conv0 up\n", "        x = self.conv_0([x, w0])\n", "        x = self.apply_noise_0(x)\n", "        x = self.apply_bias_0(x)\n", "        x = self.leaky_relu_0(x)\n\n", "        # conv block\n", "        x = self.conv_1([x, w1])\n", "        x = self.apply_noise_1(x)\n", "        x = self.apply_bias_1(x)\n", "        x = self.leaky_relu_1(x)\n", "        return x\n", "    def get_config(self):\n", "        config = super(SynthesisBlock, self).get_config()\n", "        config.update({\n", "            'in_ch': self.in_ch,\n", "            'res': self.res,\n", "            'fmaps': self.fmaps,\n", "            'gain': self.gain,\n", "            'lrmul': self.lrmul,\n", "        })\n", "        return config\n", "#\n", "class Synthesis(tf.keras.layers.Layer):\n", "    def __init__(self, resolutions, featuremaps, name, **kwargs):\n", "        super(Synthesis, self).__init__(name=name, **kwargs)\n", "        self.resolutions = resolutions\n", "        self.featuremaps = featuremaps\n", "        self.k = onmoono.setup_resample_kernel(k=[1, 3, 3, 1])\n\n", "        # initial layer\n", "        res, n_f = resolutions[0], featuremaps[0]\n", "        self.initial_block = SynthesisConstBlock(fmaps=n_f, res=res, name='{:d}x{:d}/const'.format(res, res))\n", "        self.initial_torgb = ToRGB(in_ch=n_f, name='{:d}x{:d}/ToRGB'.format(res, res))\n\n", "        # stack generator block with lerp block\n", "        prev_n_f = n_f\n", "        self.blocks = list()\n", "        self.torgbs = list()\n", "        for res, n_f in zip(self.resolutions[1:], self.featuremaps[1:]):\n", "            self.blocks.append(SynthesisBlock(in_ch=prev_n_f, fmaps=n_f, res=res,\n", "                                              name='{:d}x{:d}/block'.format(res, res)))\n", "            self.torgbs.append(ToRGB(in_ch=n_f, name='{:d}x{:d}/ToRGB'.format(res, res)))\n", "            prev_n_f = n_f\n", "    def call(self, inputs, training=None, mask=None):\n", "        w_broadcasted = inputs\n\n", "        # initial layer\n", "        w0, w1 = w_broadcasted[:, 0], w_broadcasted[:, 1]\n", "        x = self.initial_block(w0)\n", "        y = self.initial_torgb([x, w1])\n", "        layer_index = 1\n", "        for block, torgb in zip(self.blocks, self.torgbs):\n", "            w0 = w_broadcasted[:, layer_index]\n", "            w1 = w_broadcasted[:, layer_index + 1]\n", "            w2 = w_broadcasted[:, layer_index + 2]\n", "            x = block([x, w0, w1])\n", "            y = onmoono.upsample_2d(y, self.k, factor=2, gain=1.0)\n", "            y = y + torgb([x, w2])\n", "            layer_index += 2\n", "        images_out = y\n", "        return images_out\n", "    def get_config(self):\n", "        config = super(Synthesis, self).get_config()\n", "        config.update({\n", "            'resolutions': self.resolutions,\n", "            'featuremaps': self.featuremaps,\n", "            'k': self.k,\n", "        })\n", "        return config\n", "#\n", "class Generator(tf.keras.Model):\n", "    def __init__(self, g_params, **kwargs):\n", "        super(Generator, self).__init__(**kwargs)\n", "        self.z_dim = g_params['z_dim']\n", "        self.w_dim = g_params['w_dim']\n", "        self.labels_dim = g_params['labels_dim']\n", "        self.n_mapping = g_params['n_mapping']\n", "        self.resolutions = g_params['resolutions']\n", "        self.featuremaps = g_params['featuremaps']\n", "        self.w_ema_decay = g_params['w_ema_decay']\n", "        self.style_mixing_prob = g_params['style_mixing_prob']\n", "        self.n_broadcast = len(self.resolutions) * 2\n", "        self.mixing_layer_indices = np.arange(self.n_broadcast)[np.newaxis, :, np.newaxis]\n", "        self.g_mapping = Mapping(self.w_dim, self.labels_dim, self.n_mapping, name='g_mapping')\n", "        self.broadcast = tf.keras.layers.Lambda(lambda x: tf.tile(x[:, np.newaxis], [1, self.n_broadcast, 1]))\n", "        self.synthesis = Synthesis(self.resolutions, self.featuremaps, name='g_synthesis')\n", "    def build(self, input_shape):\n", "        # w_avg\n", "        self.w_avg = tf.Variable(tf.zeros(shape=[self.w_dim], dtype=tf.dtypes.float32), name='w_avg', trainable=False)\n", "    def set_as_moving_average_of(self, src_net, beta=0.99, beta_nontrainable=0.0):\n", "        def split_first_name(name):\n", "            splitted = name.split('/')\n", "            new_name = '/'.join(splitted[1:])\n", "            return new_name\n", "        for cw in self.trainable_weights:\n", "            cw_name = split_first_name(cw.name)\n", "            for sw in src_net.trainable_weights:\n", "                sw_name = split_first_name(sw.name)\n", "                if cw_name == sw_name:\n", "                    assert sw.shape == cw.shape\n", "                    cw.assign(onmoono.lerp(sw, cw, beta))\n", "                    break\n", "        for cw in self.non_trainable_weights:\n", "            cw_name = split_first_name(cw.name)\n", "            for sw in src_net.non_trainable_weights:\n", "                sw_name = split_first_name(sw.name)\n", "                if cw_name == sw_name:\n", "                    assert sw.shape == cw.shape\n", "                    cw.assign(onmoono.lerp(sw, cw, beta_nontrainable))\n", "                    break\n", "        return\n", "    def update_moving_average_of_w(self, w_broadcasted):\n", "        # compute average of current w\n", "        batch_avg = tf.reduce_mean(w_broadcasted[:, 0], axis=0)\n\n", "        # compute moving average of w and update(assign) w_avg\n", "        self.w_avg.assign(onmoono.lerp(batch_avg, self.w_avg, self.w_ema_decay))\n", "        return\n", "    def style_mixing_regularization(self, latents1, labels, w_broadcasted1):\n", "        # get another w and broadcast it\n", "        latents2 = tf.random.normal(shape=tf.shape(latents1), dtype=tf.dtypes.float32)\n", "        dlatents2 = self.g_mapping([latents2, labels])\n", "        w_broadcasted2 = self.broadcast(dlatents2)\n\n", "        # find mixing limit index\n", "        if tf.random.uniform([], 0.0, 1.0) < self.style_mixing_prob:\n", "            mixing_cutoff_index = tf.random.uniform([], 1, self.n_broadcast, dtype=tf.dtypes.int32)\n", "        else:\n", "            mixing_cutoff_index = tf.constant(self.n_broadcast, dtype=tf.dtypes.int32)\n\n", "        # mix it\n", "        mixed_w_broadcasted = tf.where(\n", "            condition=tf.broadcast_to(self.mixing_layer_indices < mixing_cutoff_index, tf.shape(w_broadcasted1)),\n", "            x=w_broadcasted1,\n", "            y=w_broadcasted2)\n", "        return mixed_w_broadcasted\n", "    def truncation_trick(self, w_broadcasted, truncation_cutoff, truncation_psi):\n", "        ones = np.ones_like(self.mixing_layer_indices, dtype=np.float32)\n", "        if truncation_cutoff is None:\n", "            truncation_coefs = ones * truncation_psi\n", "        else:\n", "            truncation_coefs = ones\n", "            for index in range(self.n_broadcast):\n", "                if index < truncation_cutoff:\n", "                    truncation_coefs[:, index, :] = truncation_psi\n", "        truncated_w_broadcasted = onmoono.lerp(self.w_avg, w_broadcasted, truncation_coefs)\n", "        return truncated_w_broadcasted\n", "    @tf.function\n", "    def call(self, inputs, truncation_cutoff=None, truncation_psi=1.0, training=None, mask=None):\n", "        latents, labels = inputs\n", "        dlatents = self.g_mapping([latents, labels])\n", "        w_broadcasted = self.broadcast(dlatents)\n", "        if training:\n", "            self.update_moving_average_of_w(w_broadcasted)\n", "            w_broadcasted = self.style_mixing_regularization(latents, labels, w_broadcasted)\n", "        if not training:\n", "            w_broadcasted = self.truncation_trick(w_broadcasted, truncation_cutoff, truncation_psi)\n", "        image_out = self.synthesis(w_broadcasted)\n", "        return image_out, w_broadcasted\n", "    def compute_output_shape(self, input_shape):\n", "        assert isinstance(input_shape, list)\n\n", "        # shape_latents, shape_labels = input_shape\n", "        return input_shape[0][0], 3, self.resolutions[-1], self.resolutions[-1]\n", "    @tf.function\n", "    def serve(self, latents, labels, truncation_psi):\n", "        dlatents = self.g_mapping([latents, labels])\n", "        w_broadcasted = self.broadcast(dlatents)\n", "        w_broadcasted = self.truncation_trick(w_broadcasted, truncation_cutoff=None, truncation_psi=truncation_psi)\n", "        image_out = self.synthesis(w_broadcasted)\n", "        image_out.set_shape([None, 3, self.resolutions[-1], self.resolutions[-1]])\n", "        return image_out\n", "#\n", "#\n", "#    -> https://github.com/moono/stylegan2-tf-2.x/tree/master/stylegan2/discriminator.py <-\n", "#\n", "#\n", "class FromRGB(tf.keras.layers.Layer):\n", "    def __init__(self, fmaps, **kwargs):\n", "        super(FromRGB, self).__init__(**kwargs)\n", "        self.fmaps = fmaps\n", "        self.conv = ResizeConv2D(fmaps=self.fmaps, kernel=1, gain=1.0, lrmul=1.0,\n", "                                 up=False, down=False, resample_kernel=None, name='conv')\n", "        self.apply_bias = Bias(lrmul=1.0, name='bias')\n", "        self.leaky_relu = LeakyReLU(name='lrelu')\n", "    def call(self, inputs, training=None, mask=None):\n", "        y = self.conv(inputs)\n", "        y = self.apply_bias(y)\n", "        y = self.leaky_relu(y)\n", "        return y\n", "#\n", "class DiscriminatorBlock(tf.keras.layers.Layer):\n", "    def __init__(self, n_f0, n_f1, res, **kwargs):\n", "        super(DiscriminatorBlock, self).__init__(**kwargs)\n", "        self.gain = 1.0\n", "        self.lrmul = 1.0\n", "        self.n_f0 = n_f0\n", "        self.n_f1 = n_f1\n", "        self.res = res\n", "        self.resnet_scale = 1. / np.sqrt(2.)\n\n", "        # conv_0\n", "        self.conv_0 = ResizeConv2D(fmaps=self.n_f0, kernel=3, gain=self.gain, lrmul=self.lrmul,\n", "                                   up=False, down=False, resample_kernel=None, name='conv_0')\n", "        self.apply_bias_0 = Bias(self.lrmul, name='bias_0')\n", "        self.leaky_relu_0 = LeakyReLU(name='lrelu_0')\n\n", "        # conv_1 down\n", "        self.conv_1 = ResizeConv2D(fmaps=self.n_f1, kernel=3, gain=self.gain, lrmul=self.lrmul,\n", "                                   up=False, down=True, resample_kernel=[1, 3, 3, 1], name='conv_1')\n", "        self.apply_bias_1 = Bias(self.lrmul, name='bias_1')\n", "        self.leaky_relu_1 = LeakyReLU(name='lrelu_1')\n\n", "        # resnet skip\n", "        self.conv_skip = ResizeConv2D(fmaps=self.n_f1, kernel=1, gain=self.gain, lrmul=self.lrmul,\n", "                                      up=False, down=True, resample_kernel=[1, 3, 3, 1], name='skip')\n", "    def call(self, inputs, training=None, mask=None):\n", "        x = inputs\n", "        residual = x\n\n", "        # conv0\n", "        x = self.conv_0(x)\n", "        x = self.apply_bias_0(x)\n", "        x = self.leaky_relu_0(x)\n\n", "        # conv1 down\n", "        x = self.conv_1(x)\n", "        x = self.apply_bias_1(x)\n", "        x = self.leaky_relu_1(x)\n\n", "        # resnet skip\n", "        residual = self.conv_skip(residual)\n", "        x = (x + residual) * self.resnet_scale\n", "        return x\n", "#\n", "class DiscriminatorLastBlock(tf.keras.layers.Layer):\n", "    def __init__(self, n_f0, n_f1, res, **kwargs):\n", "        super(DiscriminatorLastBlock, self).__init__(**kwargs)\n", "        self.gain = 1.0\n", "        self.lrmul = 1.0\n", "        self.n_f0 = n_f0\n", "        self.n_f1 = n_f1\n", "        self.res = res\n", "        self.minibatch_std = MinibatchStd(group_size=4, num_new_features=1, name='minibatchstd')\n\n", "        # conv_0\n", "        self.conv_0 = ResizeConv2D(fmaps=self.n_f0, kernel=3, gain=self.gain, lrmul=self.lrmul,\n", "                                   up=False, down=False, resample_kernel=None, name='conv_0')\n", "        self.apply_bias_0 = Bias(self.lrmul, name='bias_0')\n", "        self.leaky_relu_0 = LeakyReLU(name='lrelu_0')\n\n", "        # dense_1\n", "        self.dense_1 = Dense(self.n_f1, gain=self.gain, lrmul=self.lrmul, name='dense_1')\n", "        self.apply_bias_1 = Bias(self.lrmul, name='bias_1')\n", "        self.leaky_relu_1 = LeakyReLU(name='lrelu_1')\n", "    def call(self, x, training=None, mask=None):\n", "        x = self.minibatch_std(x)\n\n", "        # conv_0\n", "        x = self.conv_0(x)\n", "        x = self.apply_bias_0(x)\n", "        x = self.leaky_relu_0(x)\n\n", "        # dense_1\n", "        x = self.dense_1(x)\n", "        x = self.apply_bias_1(x)\n", "        x = self.leaky_relu_1(x)\n", "        return x\n", "#\n", "class Discriminator(tf.keras.Model):\n", "    def __init__(self, d_params, **kwargs):\n", "        super(Discriminator, self).__init__(**kwargs)\n", "        # discriminator's (resolutions and featuremaps) are reversed against generator's\n", "        self.labels_dim = d_params['labels_dim']\n", "        self.r_resolutions = d_params['resolutions'][::-1]\n", "        self.r_featuremaps = d_params['featuremaps'][::-1]\n\n", "        # stack discriminator blocks\n", "        res0, n_f0 = self.r_resolutions[0], self.r_featuremaps[0]\n", "        self.initial_fromrgb = FromRGB(fmaps=n_f0, name='{:d}x{:d}/FromRGB'.format(res0, res0))\n", "        self.blocks = list()\n", "        for index, (res0, n_f0) in enumerate(zip(self.r_resolutions[:-1], self.r_featuremaps[:-1])):\n", "            n_f1 = self.r_featuremaps[index + 1]\n", "            self.blocks.append(DiscriminatorBlock(n_f0=n_f0, n_f1=n_f1, res=res0, name='{:d}x{:d}'.format(res0, res0)))\n\n", "        # set last discriminator block\n", "        res = self.r_resolutions[-1]\n", "        n_f0, n_f1 = self.r_featuremaps[-2], self.r_featuremaps[-1]\n", "        self.last_block = DiscriminatorLastBlock(n_f0, n_f1, res, name='{:d}x{:d}'.format(res, res))\n\n", "        # set last dense layer\n", "        self.last_dense = Dense(max(self.labels_dim, 1), gain=1.0, lrmul=1.0, name='last_dense')\n", "        self.last_bias = Bias(lrmul=1.0, name='last_bias')\n", "    def call(self, inputs, training=None, mask=None):\n", "        images, labels = inputs\n", "        x = self.initial_fromrgb(images)\n", "        for block in self.blocks:\n", "            x = block(x)\n", "        x = self.last_block(x)\n", "        x = self.last_dense(x)\n", "        x = self.last_bias(x)\n", "        if self.labels_dim > 0:\n", "            x = tf.reduce_sum(x * labels, axis=1, keepdims=True)\n", "        scores_out = x\n", "        return scores_out\n", "#\n", "#\n", "#    -> https://github.com/moono/stylegan2-tf-2.x/tree/master/stylegan2/train_advanced.py <-\n", "#\n", "#\n", "class Trainer(object):\n", "    def __init__(self, t_params, name):\n", "        self.model_base_dir = t_params['model_base_dir']\n", "        self.tfrecord_dir = t_params['tfrecord_dir']\n", "        self.shuffle_buffer_size = t_params['shuffle_buffer_size']\n", "        self.g_params = t_params['g_params']\n", "        self.d_params = t_params['d_params']\n", "        self.g_opt = t_params['g_opt']\n", "        self.d_opt = t_params['d_opt']\n", "        self.batch_size = t_params['batch_size']\n", "        self.n_total_image = t_params['n_total_image']\n", "        self.n_samples = min(t_params['batch_size'], t_params['n_samples'])\n", "        self.lazy_regularization = t_params['lazy_regularization']\n", "        self.r1_gamma = 10.0\n", "        # self.r2_gamma = 0.0\n", "        self.max_steps = int(np.ceil(self.n_total_image / self.batch_size))\n", "        self.out_res = self.g_params['resolutions'][-1]\n", "        self.log_template = 'step {}: elapsed: {:.2f}s, d_loss: {:.3f}, g_loss: {:.3f}, r1_reg: {:.3f}, pl_reg: {:.3f}'\n", "        self.print_step = 10\n", "        self.save_step = 100\n", "        self.image_summary_step = 100\n", "        self.reached_max_steps = False\n\n", "        # setup optimizer params\n", "        self.g_opt = self.set_optimizer_params(self.g_opt)\n", "        self.d_opt = self.set_optimizer_params(self.d_opt)\n", "        self.pl_mean = tf.Variable(initial_value=0.0, name='pl_mean', trainable=False)\n", "        self.pl_decay = 0.01\n", "        self.pl_weight = 1.0\n", "        self.pl_denorm = 1.0 / np.sqrt(self.out_res * self.out_res)\n\n", "        # grab dataset\n", "        print('|... Trainer setting datasets')\n", "        self.dataset = get_ffhq_dataset(self.tfrecord_dir, self.out_res, self.shuffle_buffer_size,\n", "                                        self.batch_size, epochs=None)\n\n", "        # create models\n", "        print('|... Trainer  Create models')\n", "        self.generator = Generator(self.g_params)\n", "        self.discriminator = Discriminator(self.d_params)\n", "        self.d_optimizer = tf.keras.optimizers.Adam(self.d_opt['learning_rate'],\n", "                                                    beta_1=self.d_opt['beta1'],\n", "                                                    beta_2=self.d_opt['beta2'],\n", "                                                    epsilon=self.d_opt['epsilon'])\n", "        self.g_optimizer = tf.keras.optimizers.Adam(self.g_opt['learning_rate'],\n", "                                                    beta_1=self.g_opt['beta1'],\n", "                                                    beta_2=self.g_opt['beta2'],\n", "                                                    epsilon=self.g_opt['epsilon'])\n", "        self.g_clone = Generator(self.g_params)\n\n", "        # finalize model (build)\n", "        test_latent = np.ones((1, self.g_params['z_dim']), dtype=np.float32)\n", "        test_labels = np.ones((1, self.g_params['labels_dim']), dtype=np.float32)\n", "        test_images = np.ones((1, 3, self.out_res, self.out_res), dtype=np.float32)\n", "        _, __ = self.generator([test_latent, test_labels], training=False)\n", "        _ = self.discriminator([test_images, test_labels], training=False)\n", "        _, __ = self.g_clone([test_latent, test_labels], training=False)\n", "        print('Copying g_clone')\n", "        self.g_clone.set_weights(self.generator.get_weights())\n\n", "        # setup saving locations (object based savings)\n", "        self.ckpt_dir = os.path.join(self.model_base_dir, name)\n", "        self.ckpt = tf.train.Checkpoint(d_optimizer=self.d_optimizer,\n", "                                        g_optimizer=self.g_optimizer,\n", "                                        discriminator=self.discriminator,\n", "                                        generator=self.generator,\n", "                                        g_clone=self.g_clone)\n", "        self.manager = tf.train.CheckpointManager(self.ckpt, self.ckpt_dir, max_to_keep=2)\n\n", "        # try to restore\n", "        self.ckpt.restore(self.manager.latest_checkpoint)\n", "        if self.manager.latest_checkpoint:\n", "            print('Restored from {}'.format(self.manager.latest_checkpoint))\n\n", "            # check if already trained in this resolution\n", "            restored_step = self.g_optimizer.iterations.numpy()\n", "            if restored_step >= self.max_steps:\n", "                print('Already reached max steps {}/{}'.format(restored_step, self.max_steps))\n", "                self.reached_max_steps = True\n", "                return\n", "        else:\n", "            print('Not restoring from saved checkpoint')\n", "    def set_optimizer_params(self, params):\n", "        if self.lazy_regularization:\n", "            mb_ratio = params['reg_interval'] / (params['reg_interval'] + 1)\n", "            params['learning_rate'] = params['learning_rate'] * mb_ratio\n", "            params['beta1'] = params['beta1'] ** mb_ratio\n", "            params['beta2'] = params['beta2'] ** mb_ratio\n", "        return params\n", "    @tf.function\n", "    def d_train_step(self, z, real_images, labels):\n", "        with tf.GradientTape() as d_tape:\n", "            # forward pass\n", "            fake_images, _ = self.generator([z, labels], training=True)\n", "            real_scores = self.discriminator([real_images, labels], training=True)\n", "            fake_scores = self.discriminator([fake_images, labels], training=True)\n\n", "            # gan loss\n", "            d_loss = tf.math.softplus(fake_scores)\n", "            d_loss += tf.math.softplus(-real_scores)\n", "            d_loss = tf.reduce_mean(d_loss)\n", "        d_gradients = d_tape.gradient(d_loss, self.discriminator.trainable_variables)\n", "        self.d_optimizer.apply_gradients(zip(d_gradients, self.discriminator.trainable_variables))\n", "        return d_loss\n", "    @tf.function\n", "    def d_reg_train_step(self, z, real_images, labels):\n", "        with tf.GradientTape() as d_tape:\n", "            # forward pass\n", "            fake_images, _ = self.generator([z, labels], training=True)\n", "            real_scores = self.discriminator([real_images, labels], training=True)\n", "            fake_scores = self.discriminator([fake_images, labels], training=True)\n\n", "            # gan loss\n", "            d_loss = tf.math.softplus(fake_scores)\n", "            d_loss += tf.math.softplus(-real_scores)\n\n", "            # simple GP\n", "            with tf.GradientTape() as p_tape:\n", "                p_tape.watch(real_images)\n", "                real_loss = tf.reduce_sum(self.discriminator([real_images, labels], training=True))\n", "            real_grads = p_tape.gradient(real_loss, real_images)\n", "            r1_penalty = tf.reduce_sum(tf.math.square(real_grads), axis=[1, 2, 3])\n", "            r1_penalty = tf.expand_dims(r1_penalty, axis=1)\n", "            r1_penalty = r1_penalty * self.d_opt['reg_interval']\n\n", "            # combine\n", "            d_loss += r1_penalty * (0.5 * self.r1_gamma)\n", "            d_loss = tf.reduce_mean(d_loss)\n", "        d_gradients = d_tape.gradient(d_loss, self.discriminator.trainable_variables)\n", "        self.d_optimizer.apply_gradients(zip(d_gradients, self.discriminator.trainable_variables))\n", "        return d_loss, tf.reduce_mean(r1_penalty)\n", "    @tf.function\n", "    def g_train_step(self, z, labels):\n", "        with tf.GradientTape() as g_tape:\n", "            # forward pass\n", "            fake_images, _ = self.generator([z, labels], training=True)\n", "            fake_scores = self.discriminator([fake_images, labels], training=True)\n\n", "            # gan loss\n", "            g_loss = tf.math.softplus(-fake_scores)\n", "            g_loss = tf.reduce_mean(g_loss)\n", "        g_gradients = g_tape.gradient(g_loss, self.generator.trainable_variables)\n", "        self.g_optimizer.apply_gradients(zip(g_gradients, self.generator.trainable_variables))\n", "        return g_loss\n\n", "    # @tf.function\n", "    def g_reg_train_step(self, z, labels):\n", "        with tf.GradientTape() as g_tape:\n", "            # forward pass\n", "            fake_images, _ = self.generator([z, labels], training=True)\n", "            fake_scores = self.discriminator([fake_images, labels], training=True)\n\n", "            # gan loss\n", "            g_loss = tf.math.softplus(-fake_scores)\n\n", "            # path length regularization\n", "            # Compute |J*y|.\n", "            with tf.GradientTape() as pl_tape:\n", "                fake_images, w_broadcasted = self.generator([z, labels], training=True)\n", "                pl_noise = tf.random.normal(tf.shape(fake_images), mean=0.0, stddev=1.0, dtype=tf.float32) * self.pl_denorm\n", "                pl_noise_added = tf.reduce_sum(fake_images * pl_noise)\n", "            \n", "            print(\"|... g_reg_train_step ***************************************>\")\n", "            #  pl_noise_added:  tf.Tensor(2.973158, shape=(), dtype=float32)\n", "            #  w_broadcasted:   tf.Tensor([[[-0.25851128  0.28737155  1.8203238  ... -0.5302262  -0.00633283\n", "            pl_grads = pl_tape.gradient(pl_noise_added, w_broadcasted) # None\n", "            print(f\"|... g_reg_train_step: pl_grads {pl_grads}\")\n\n", "            # ***************************************************************************\n", "            pl_lengths = 0.0                                # _e_ ***********************\n", "            if pl_grads:\n", "                pl_lengths = tf.math.sqrt(tf.reduce_mean(tf.reduce_sum(tf.math.square(pl_grads), axis=2), axis=1))\n", "            # ***************************************************************************\n\n", "            # Track exponential moving average of |J*y|.\n", "            pl_mean_val = self.pl_mean + self.pl_decay * (tf.reduce_mean(pl_lengths) - self.pl_mean)\n", "            self.pl_mean.assign(pl_mean_val)\n\n", "            # Calculate (|J*y|-a)^2.\n", "            pl_penalty = tf.square(pl_lengths - self.pl_mean)\n\n", "            # pl_penalty = pl_reg(fake_images, w_broadcasted, self.pl_mean)\n", "            # print(f\"*** pl_penalty {pl_penalty}\")\n\n", "            # compute\n", "            _pl_reg = pl_penalty * self.pl_weight\n", "            print(f\"*** _pl_reg {_pl_reg}\")\n", "            print(\"|... g_reg_train_step  <***********************************\")\n\n", "            # combine\n", "            g_loss += _pl_reg\n", "            g_loss = tf.reduce_mean(g_loss)\n", "        g_gradients = g_tape.gradient(g_loss, self.generator.trainable_variables)\n", "        self.g_optimizer.apply_gradients(zip(g_gradients, self.generator.trainable_variables))\n", "        return g_loss, tf.reduce_mean(_pl_reg)\n", "    def train(self):\n", "        if self.reached_max_steps:\n", "            return\n\n", "        # start actual training\n", "        print('Start Training')\n\n", "        # setup tensorboards\n", "        train_summary_writer = tf.summary.create_file_writer(self.ckpt_dir)\n\n", "        # loss metrics\n", "        metric_g_loss = tf.keras.metrics.Mean('g_loss', dtype=tf.float32)\n", "        metric_d_loss = tf.keras.metrics.Mean('d_loss', dtype=tf.float32)\n", "        metric_r1_reg = tf.keras.metrics.Mean('r1_reg', dtype=tf.float32)\n", "        metric_pl_reg = tf.keras.metrics.Mean('pl_reg', dtype=tf.float32)\n\n", "        # start training\n", "        print('max_steps: {}'.format(self.max_steps))\n", "        losses = {'g_loss': 0.0, 'd_loss': 0.0, 'r1_reg': 0.0, 'pl_reg': 0.0}\n", "        t_start = time.time()\n", "        for real_images in self.dataset:\n", "            # preprocess inputs\n", "            z = tf.random.normal(shape=[tf.shape(real_images)[0], self.g_params['z_dim']], dtype=tf.dtypes.float32)\n", "            real_images = onmoono.preprocess_fit_train_image(real_images, self.out_res)\n", "            labels = tf.ones((tf.shape(real_images)[0], self.g_params['labels_dim']), dtype=tf.dtypes.float32)\n\n", "            # get current step\n", "            step = self.g_optimizer.iterations.numpy()\n\n", "            # d train step\n", "            if step % self.d_opt['reg_interval'] == 0:\n", "                d_loss, r1_reg = self.d_reg_train_step(z, real_images, labels)\n", "                # update values for printing\n", "                losses['d_loss'] = d_loss.numpy()\n", "                losses['r1_reg'] = r1_reg.numpy()\n", "                # update metrics\n", "                metric_d_loss(d_loss)\n", "                metric_r1_reg(r1_reg)\n", "            else:\n", "                d_loss = self.d_train_step(z, real_images, labels)\n", "                # update values for printing\n", "                losses['d_loss'] = d_loss.numpy()\n", "                losses['r1_reg'] = 0.0\n", "                # update metrics\n", "                metric_d_loss(d_loss)\n\n", "            # update g_clone\n", "            self.g_clone.set_as_moving_average_of(self.generator)\n\n", "            # g train step\n", "            if step % self.g_opt['reg_interval'] == 0:\n", "                g_loss, pl_reg = self.g_reg_train_step(z, labels)\n", "                # update values for printing\n", "                losses['g_loss'] = g_loss.numpy()\n", "                losses['pl_reg'] = pl_reg.numpy()\n", "                # update metrics\n", "                metric_g_loss(g_loss)\n", "                metric_pl_reg(pl_reg)\n", "            else:\n", "                g_loss = self.g_train_step(z, labels)\n", "                # update values for printing\n", "                losses['g_loss'] = g_loss.numpy()\n", "                losses['pl_reg'] = 0.0\n", "                # update metrics\n", "                metric_g_loss(g_loss)\n\n", "            # save to tensorboard\n", "            with train_summary_writer.as_default():\n", "                tf.summary.scalar('g_loss', metric_g_loss.result(), step=step)\n", "                tf.summary.scalar('d_loss', metric_d_loss.result(), step=step)\n", "                tf.summary.scalar('r1_reg', metric_r1_reg.result(), step=step)\n", "                tf.summary.scalar('pl_reg', metric_pl_reg.result(), step=step)\n", "                tf.summary.histogram('w_avg', self.generator.w_avg, step=step)\n\n", "            # save every self.save_step\n", "            if step % self.save_step == 0:\n", "                self.manager.save(checkpoint_number=step)\n\n", "            # save every self.image_summary_step\n", "            if step % self.image_summary_step == 0:\n", "                # add summary image\n", "                summary_image = self.sample_images_tensorboard(real_images)\n", "                with train_summary_writer.as_default():\n", "                    tf.summary.image('images', summary_image, step=step)\n\n", "            # print every self.print_steps\n", "            if step % self.print_step == 0:\n", "                elapsed = time.time() - t_start\n", "                print(self.log_template.format(step, elapsed,\n", "                        losses['d_loss'], losses['g_loss'], losses['r1_reg'], losses['pl_reg']))\n", "                # reset timer\n", "                t_start = time.time()\n\n", "            # check exit status\n", "            if step >= self.max_steps:\n", "                break\n\n", "        # get current step\n", "        step = self.g_optimizer.iterations.numpy()\n", "        elapsed = time.time() - t_start\n", "        print(self.log_template.format(step, elapsed,\n", "                        losses['d_loss'], losses['g_loss'], losses['r1_reg'], losses['pl_reg']))\n\n", "        # save last checkpoint\n", "        self.manager.save(checkpoint_number=step)\n", "        return\n", "    def sample_images_tensorboard(self, real_images):\n", "        # prepare inputs\n", "        reals = real_images[:self.n_samples, :, :, :]\n", "        latents = tf.random.normal(shape=(self.n_samples, self.g_params['z_dim']), dtype=tf.dtypes.float32)\n", "        dummy_labels = tf.ones((self.n_samples, self.g_params['labels_dim']), dtype=tf.dtypes.float32)\n\n", "        # run networks\n", "        fake_images_00, _ = self.g_clone([latents, dummy_labels], truncation_psi=0.0, training=False)\n", "        fake_images_05, _ = self.g_clone([latents, dummy_labels], truncation_psi=0.5, training=False)\n", "        fake_images_07, _ = self.g_clone([latents, dummy_labels], truncation_psi=0.7, training=False)\n", "        fake_images_10, _ = self.g_clone([latents, dummy_labels], truncation_psi=1.0, training=False)\n\n", "        # merge on batch dimension: [5 * n_samples, 3, out_res, out_res]\n", "        out = tf.concat([reals, fake_images_00, fake_images_05, fake_images_07, fake_images_10], axis=0)\n\n", "        # prepare for image saving: [5 * n_samples, out_res, out_res, 3]\n", "        out = onmoono.postprocess_images(out)\n\n", "        # resize to save disk spaces: [5 * n_samples, size, size, 3]\n", "        size = min(self.out_res, 256)\n", "        out = tf.image.resize(out, size=[size, size])\n\n", "        # make single image and add batch dimension for tensorboard: [1, 5 * size, n_samples * size, 3]\n", "        out = onmoono.merge_batch_images(out, size, rows=5, cols=self.n_samples)\n", "        out = np.expand_dims(out, axis=0)\n", "        return out\n", "#\n", "#\n", "#   FUNS\n", "#\n", "#\n", "def filter_resolutions_featuremaps(resolutions, featuremaps, res):\n", "    index = resolutions.index(res)\n", "    filtered_resolutions = resolutions[:index + 1]\n", "    filtered_featuremaps = featuremaps[:index + 1]\n", "    return filtered_resolutions, filtered_featuremaps\n", "#\n", "#\n", "#    -> https://github.com/moono/stylegan2-tf-2.x/tree/master/stylegan2/dataset_ffhq.py <-\n", "#\n", "def parse_tfrecord_tf(record):\n", "    # n_samples = 70000\n", "    features = tf.io.parse_single_example(record, features={\n", "        'shape': tf.io.FixedLenFeature([3], tf.dtypes.int64),\n", "        'data': tf.io.FixedLenFeature([], tf.dtypes.string)\n", "    })\n\n", "    # [0 ~ 255] uint8\n", "    images = tf.io.decode_raw(features['data'], tf.dtypes.uint8)\n", "    images = tf.reshape(images, features['shape'])\n\n", "    # [0.0 ~ 255.0] float32\n", "    images = tf.cast(images, tf.dtypes.float32)\n", "    return images\n", "#\n", "def get_ffhq_dataset(tfrecord_base_dir, res, buffer_size, batch_size, epochs=None):\n", "    fn_index = int(np.log2(res))\n", "    tfrecord_fn = os.path.join(tfrecord_base_dir, 'ffhq-r{:02d}.tfrecords'.format(fn_index))\n", "    with tf.device('/cpu:0'):\n", "        dataset = tf.data.TFRecordDataset(tfrecord_fn)\n", "        dataset = dataset.map(map_func=parse_tfrecord_tf, num_parallel_calls=8)\n", "        dataset = dataset.shuffle(buffer_size=buffer_size)\n", "        dataset = dataset.repeat(epochs)\n", "        dataset = dataset.batch(batch_size)\n", "        dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n", "    return dataset\n", "#\n", "#\n", "#   CMDS\n", "#\n", "#\n", "# https://github.com/moono/stylegan2-tf-2.x\n", "#   [1] weight modulation / demodulation\n", "#   [2] skip architecture\n", "#   [3] resnet architecture\n", "#   [4] path regularization\n", "#   [5] lazy regularization\n", "#   [6] single GPU training\n", "#   [7] inference from official Generator weights\n", "#\n", "def nnutils(args, kwargs):\n", "    args = onutil.pargs(vars(args))\n", "    onutil.ddict(vars(args), 'args')\n", "    print(f'|===> nnutils:  \\n \\\n", "            cwd: {os.getcwd()} \\n \\\n", "    ')\n", "    if 1: # git\n", "        onutil.get_git(args.AUTHOR, args.GITPOD, args.proj_dir)\n", "    if 1: # rgb to nba\n", "        print(f'|===> adjust_dynamic_range')\n", "        out_dtype = tf.dtypes.float32\n", "        range_in = (0.0, 255.0)\n", "        range_out = (-1.0, 1.0)\n", "        # images = np.ones((2, 2), dtype=np.float32) * 255.0\n", "        images1 = np.ones((2, 2), dtype=np.float32) * 0.0\n", "        images2 = np.ones((2, 2), dtype=np.float32) * 127.5\n", "        images3 = np.ones((2, 2), dtype=np.float32) * 255.0\n", "        adjusted1 = onmoono.adjust_dynamic_range(images1, range_in, range_out, out_dtype)\n", "        adjusted2 = onmoono.adjust_dynamic_range(images2, range_in, range_out, out_dtype)\n", "        adjusted3 = onmoono.adjust_dynamic_range(images3, range_in, range_out, out_dtype)\n", "        print(f'|.... adjust_dynamic_range 1: {adjusted1}')\n", "        print(f'|.... adjust_dynamic_range 2: {adjusted2}')\n", "        print(f'|.... adjust_dynamic_range 3: {adjusted3}')\n", "    \n", "    if 1: # nba to rgb\n", "        out_dtype = tf.dtypes.uint8\n", "        range_in = (-1.0, 1.0)\n", "        range_out = (0.0, 255.0)\n", "        images1 = np.ones((2, 2), dtype=np.float32) * -1.0\n", "        images2 = np.ones((2, 2), dtype=np.float32) * 0.0\n", "        images3 = np.ones((2, 2), dtype=np.float32) * 1.0\n", "        adjusted1 = onmoono.adjust_dynamic_range(images1, range_in, range_out, out_dtype)\n", "        adjusted2 = onmoono.adjust_dynamic_range(images2, range_in, range_out, out_dtype)\n", "        adjusted3 = onmoono.adjust_dynamic_range(images3, range_in, range_out, out_dtype)\n", "        print(f'|.... adjust_dynamic_range 1: {adjusted1}')\n", "        print(f'|.... adjust_dynamic_range 2: {adjusted2}')\n", "        print(f'|.... adjust_dynamic_range 3: {adjusted3}')\n", "    if 1: # merge\n", "        batch_size = 8\n", "        res = 128\n", "        fake_images = np.ones(shape=(batch_size, res, res, 3), dtype=np.uint8)\n", "        canvas = onmoono.merge_batch_images(fake_images, res, rows=4, cols=2)\n", "        print(f'|.... canvas shape {np.shape(canvas)}')\n", "#\n", "def nngen(args, kwargs):\n", "    args = onutil.pargs(vars(args))\n", "    onutil.ddict(vars(args), 'args')\n", "    print(f'|===> nngen: show generator model and layers \\n \\\n", "            cwd: {os.getcwd()} \\n \\\n", "    ')\n", "    batch_size = 4\n", "    g_params_with_label = {\n", "        'z_dim': 512,\n", "        'w_dim': 512,\n", "        'labels_dim': 0,\n", "        'n_mapping': 8,\n", "        'resolutions': [4, 8, 16, 32, 64, 128, 256, 512, 1024],\n", "        'featuremaps': [512, 512, 512, 512, 512, 256, 128, 64, 32],\n", "        'w_ema_decay': 0.995,\n", "        'style_mixing_prob': 0.9,\n", "        'truncation_psi': 0.5,\n", "        'truncation_cutoff': None,\n", "    }\n", "    test_z = np.ones((batch_size, g_params_with_label['z_dim']), dtype=np.float32)\n", "    test_y = np.ones((batch_size, g_params_with_label['labels_dim']), dtype=np.float32)\n", "    generator = Generator(g_params_with_label)\n", "    fake_images1, _ = generator([test_z, test_y], training=True)\n", "    fake_images2, _ = generator([test_z, test_y], training=False)\n", "    generator.summary()\n", "    print(f'|... fake_images1.shape: {fake_images1.shape}')\n", "    for v in generator.variables:\n", "        print(f'|... generator.variables: {v.name}: {v.shape}')\n", "#\n", "def nndisc(args, kwargs):\n", "    args = onutil.pargs(vars(args))\n", "    onutil.ddict(vars(args), 'args')\n", "    print(f'|===> nndisr: : show discriminator model and layers \\n \\\n", "            cwd: {os.getcwd()}  \\n \\\n", "    ')\n", "    batch_size = 4\n", "    d_params_with_label = {\n", "        'labels_dim': 0,\n", "        'resolutions': [4, 8, 16, 32, 64, 128, 256, 512, 1024],\n", "        'featuremaps': [512, 512, 512, 512, 512, 256, 128, 64, 32],\n", "    }\n", "    input_res = d_params_with_label['resolutions'][-1]\n", "    test_images = np.ones((batch_size, 3, input_res, input_res), dtype=np.float32)\n", "    test_labels = np.ones((batch_size, d_params_with_label['labels_dim']), dtype=np.float32)\n", "    discriminator = Discriminator(d_params_with_label)\n", "    scores1 = discriminator([test_images, test_labels], training=True)\n", "    scores2 = discriminator([test_images, test_labels], training=False)\n", "    discriminator.summary()\n", "    print(f'|... scores1.shape: {scores1.shape}')\n", "    print(f'|... scores2.shape: {scores2.shape}')\n", "    print()\n", "    for v in discriminator.variables:\n", "        print(f'|... discriminator.variables: {v.name}: {v.shape}')\n", "#\n", "def nntrain(args, kwargs):\n", "    # https://github.com/moono/stylegan2-tf-2.x/blob/master/dataset_ffhq.py\n", "    args = onutil.pargs(vars(args))\n", "    onutil.ddict(vars(args), 'args')\n", "    if 1: # tree\n", "        args.tfrecord_dir = os.path.join(args.gdata, 'ffhq')\n", "        args.model_base_dir=os.path.join(args.proto_dir, 'models')\n", "        os.makedirs(args.model_base_dir, exist_ok=True)\n", "    print(f'|===> nntrain: train \\n \\\n", "            args.tfrecord_dir: {args.tfrecord_dir}, \\n \\\n", "            args.model_base_dir: {args.model_base_dir}, \\n \\\n", "    ')\n", "    if 1: # test tfrecords\n", "        n_samples = 70000\n", "        res = 32\n", "        batch_size = 4\n", "        epochs = 1\n", "        dataset = get_ffhq_dataset(args.tfrecord_dir, res, batch_size, epochs)\n", "        for real_images in dataset.take(batch_size):\n", "            print(real_images.shape)  # => (1, 3, 32, 32) ...\n", "    if 1: # network params\n", "        args.train_res=32\n", "        args.shuffle_buffer_size=1000\n", "        args.batch_size=4\n", "        train_res = args.train_res\n", "        resolutions = [  4,   8,  16,  32,  64, 128, 256, 512, 1024]\n", "        featuremaps = [512, 512, 512, 512, 512, 256, 128,  64,   32]\n", "        train_resolutions, train_featuremaps = filter_resolutions_featuremaps(resolutions, featuremaps, train_res)\n", "        g_params = {\n", "            'z_dim': 512,\n", "            'w_dim': 512,\n", "            'labels_dim': 0,\n", "            'n_mapping': 8,\n", "            'resolutions': train_resolutions,\n", "            'featuremaps': train_featuremaps,\n", "            'w_ema_decay': 0.995,\n", "            'style_mixing_prob': 0.9,\n", "        }\n", "        d_params = {\n", "            'labels_dim': 0,\n", "            'resolutions': train_resolutions,\n", "            'featuremaps': train_featuremaps,\n", "        }\n\n", "        # training parameters\n", "        training_parameters = {\n", "            # global params\n", "            # **args,\n", "            'model_base_dir': args.model_base_dir,\n", "            'tfrecord_dir': args.tfrecord_dir,\n", "            'shuffle_buffer_size': args.shuffle_buffer_size,\n\n", "            # network params\n", "            'g_params': g_params,\n", "            'd_params': d_params,\n\n", "            # training params\n", "            'g_opt': {'learning_rate': 0.002, 'beta1': 0.0, 'beta2': 0.99, 'epsilon': 1e-08, 'reg_interval': 4},\n", "            'd_opt': {'learning_rate': 0.002, 'beta1': 0.0, 'beta2': 0.99, 'epsilon': 1e-08, 'reg_interval': 16},\n", "            'batch_size': args.batch_size,\n", "            'n_total_image': 25000000,\n", "            'n_samples': 4,\n", "            'lazy_regularization': True,\n", "        }\n", "    onutil.ddict(training_parameters, 'training_parameters')\n", "    if 1: # train\n", "        trainer = Trainer(\n", "            training_parameters, \n", "            name='stylegan2-ffhq'\n", "        )\n", "        trainer.train()\n", "#\n", "#\n", "#\n", "#   MAIN\n", "#\n", "#\n", "def main():\n", "    parser = argparse.ArgumentParser(description='Run \"python %(prog)s <subcommand> --help\" for subcommand help.')\n", "    onutil.dodrive()\n", "    ap = getap()\n", "    for p in ap:\n", "        cls = type(ap[p])\n", "        parser.add_argument('--'+p, type=cls, default=ap[p])\n", "    cmds = [key for key in globals() if key.startswith(\"nn\")]\n", "    primecmd = ap[\"primecmd\"]\n", "        \n", "    # ---------------------------------------------------------------\n", "    #   add subparsers\n", "    #\n", "    subparsers = parser.add_subparsers(help='subcommands', dest='command') # command - subparser\n", "    for cmd in cmds:\n", "        subparser = subparsers.add_parser(cmd, help='cmd')  # add subcommands\n", "    \n", "    subparsers_actions = [action for action in parser._actions\n", "        if isinstance(action, argparse._SubParsersAction)] # retrieve subparsers from parser\n", "  \n", "    for subparsers_action in subparsers_actions:  # add common       \n", "        for choice, subparser in subparsers_action.choices.items(): # get all subparsers and print help\n", "            for p in {}:  # subcommand args dict\n", "                cls = type(ap[p])\n", "                subparser.add_argument('--'+p, type=cls, default=ap[p])\n\n", "    # get args to pass to nn cmds\n", "    if onutil.incolab():\n", "        args = parser.parse_args('') #  defaults as args\n", "    else:\n", "        args = parser.parse_args() #  parse_arguments()\n", "    kwargs = vars(args)\n", "    subcmd = kwargs.pop('command')      \n", "    if subcmd is None:\n", "        print (f\"Missing subcommand. set to default {primecmd}\")\n", "        subcmd = primecmd\n", "    \n", "    for name in cmds:\n", "        if (subcmd == name):\n", "            print(f'|===> call {name}')\n", "            globals()[name](args, kwargs) # pass args to nn cmd\n", "#\n", "#\n", "#\n", "# python base/base.py nninfo\n", "if __name__ == \"__main__\":\n", "    print(\"|===>\", __name__)\n", "    main()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}