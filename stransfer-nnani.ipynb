{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# -*- coding: utf-8 -*-\n", "# \n", "# # Copyright (c) 2019, NVIDIA Corporation. All rights reserved.\n", "#\n", "import os\n", "import io\n", "from io import StringIO\n", "import time\n", "import argparse\n", "import functools\n", "import errno\n", "import scipy\n", "import scipy.io\n", "import requests\n", "import zipfile\n", "import random\n", "import datetime\n", "#\n", "from functools import partial\n", "from importlib import import_module\n", "#\n", "import logging\n", "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n", "#\n", "import numpy as np\n", "from numpy import *\n", "#\n", "import math\n", "from math import floor, log2\n", "from random import random\n", "from pylab import *\n", "from IPython.core.display import display\n", "import PIL\n", "from PIL import Image\n", "PIL.Image.MAX_IMAGE_PIXELS = 933120000\n", "#\n", "import scipy.ndimage as pyimg\n", "import cv2\n", "import imageio\n", "import glob\n", "import matplotlib as mpl\n", "import matplotlib.pyplot as plt \n", "import matplotlib.image as mgimg\n", "import matplotlib.animation as anim\n", "mpl.rcParams['figure.figsize'] = (12,12)\n", "mpl.rcParams['axes.grid'] = False\n", "#\n", "import shutil\n", "import gdown\n", "#\n", "import sys\n", "#\n", "import tensorflow as tf \n", "from tensorflow.keras import initializers, regularizers, constraints\n", "from tensorflow.keras import backend as K\n", "from tensorflow.keras import layers\n", "from tensorflow.keras.layers import Layer, InputSpec\n", "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D, UpSampling2D, Conv2D\n", "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, LeakyReLU\n", "from tensorflow.keras.models import Sequential, Model\n", "from tensorflow.keras.optimizers import Adam\n", "from tensorflow.python.keras.utils import conv_utils\n", "#\n", "from tensorflow.keras.layers import Lambda\n", "from tensorflow.keras.layers import add\n", "from tensorflow.keras.layers import AveragePooling2D\n", "from tensorflow.keras.initializers import VarianceScaling\n", "from tensorflow.keras.models import clone_model\n", "from tensorflow.keras.models import model_from_json\n", "#\n", "from absl import app\n", "from absl import flags\n", "from absl import logging\n", "#\n", "tf.get_logger().setLevel('ERROR')\n", "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n", "#\n", "print(f'|===> {tf.__version__}')\n", "#\n", "if 1: # get base.py from github\n", "    cwd = os.getcwd()\n", "    base_path = os.path.join(cwd, 'base.py')\n", "    if not os.path.exists(base_path):\n", "        base_file = 'base.py'\n", "        urlfolder = 'https://raw.githubusercontent.com/sifbuilder/pylon/master/'\n", "        url = f'{urlfolder}{base_file}'\n", "        print(f\"|===> nnimg: get base file \\n \\\n", "            urlfolder: {urlfolder} \\n \\\n", "            url: {url} \\n \\\n", "            base_path: {base_path} \\n \\\n", "        \")\n", "        tofile = tf.keras.utils.get_file(f'{base_path}', origin=url, extract=True)\n", "    else:\n", "        print(f\"|===> base in cwd {cwd}\")\n", "#\n", "#\n", "#   FUNS\n", "#\n", "#\n", "# check if base.Onpyon is defined\n", "try:\n", "    var = Onpyon()\n", "except NameError:\n", "    sys.path.append('../')  # if called from eon, modules are in parallel folder\n", "    sys.path.append('./')  #  if called from dnns, modules are in folder\n", "    from base import *\n", "#\n", "onutil = Onutil()\n", "onplot = Onplot()\n", "onformat = Onformat()\n", "onfile = Onfile()\n", "onvid = Onvid()\n", "onimg = Onimg()\n", "ondata = Ondata()\n", "onset = Onset()\n", "onrecord = Onrecord()\n", "ontree = Ontree()\n", "onvgg = Onvgg()\n", "onlllyas = Onlllyas()\n", "#\n", "#\n", "#   CONTEXT\n", "#\n", "#\n", "def getap():\n", "    cp = {\n", "        \"primecmd\": \"nnani\",        \n", "        \"MNAME\": \"stransfer\",      \n", "        \"AUTHOR\": \"xueyangfu\",      \n", "        \"PROJECT\": \"building\",      \n", "        \"GITPOD\": \"neural-style-tf\",      \n", "        \"DATASET\": \"styler\",        \n", "    \n", "        \"RESETCODE\": 0,\n", "        \"LOCALDATA\": 0,\n", "        \"LOCALMODELS\": 0,\n", "        \"LOCALLAB\": 1,\n", "        \"grel_infix\": '../..',            # relative path to content \n", "        \"net_prefix\": '//enas/hdrive',     \n", "        \"gdrive_prefix\": '/content/drive/My Drive',     \n", "        \"gcloud_prefix\": '/content',     \n", "    }\n", "    local_prefix = os.path.abspath('')\n", "    try:\n", "            local_prefix = os.path.dirname(os.path.realpath(__file__)) # script dir\n", "    except:\n", "            pass\n", "    cp[\"local_prefix\"] = local_prefix\n", "    tree = ontree.tree(cp)\n", "    for key in tree.keys():\n", "        cp[key] = tree[key]\n", "    hp = {\n", "        \"verbose\": 0,\n", "        \"visual\": 1,\n", "    }\n", "    ap = {}\n", "    for key in cp.keys():\n", "        ap[key] = cp[key]\n", "    for key in hp.keys():\n", "        ap[key] = hp[key]\n", "    return ap\n", "#\n", "def getxp(cp):\n", "    yp = {\n", "        # \"model_weights\": os.path.join(cp[\"gmodel\"], 'vgg', 'vgg19_weights_tf_dim_ordering_tf_kernels.h5'),\n", "        \"model_weights\": os.path.join(cp[\"gmodel\"], 'vgg', 'vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'), # vgg19\n", "        \"model_include_top\": 0,\n", "        \"model_classes\": 1000,\n", "        \"max_epochs\": 80, # 800, # 10,\n", "        \"steps_per_epoch\": 1,\n", "        \"print_iterations\": 10, #  50\n", "        \"max_iterations\": 100, # 1000\n\n", "        # \"optimizer\": \"Adam\", # \"SGD\", # \n", "        \"adam_learning_rate\": 0.01,\n", "        \"adam_beta_1\": 0.99,\n", "        \"adam_epsilon\": 1e-1,\n", "        \"optimizer\": \"SGD\", # \"Adam\", # \"SGD\", # \n", "        \"sgd_learning_rate\": 0.01, \n", "        \"learning_rate\": 1e0,\n", "        \"model_pooling\": 'avg',\n", "        \"exp_decay\": [100., 100, 0.96], # [initial_learning_rate, decay_steps, decay_rate]\n", "        # \"momentum\": 0.0, \n", "        # \"nesterov\": 0\n", "        \"content_imgs_files\": [],\n", "        \"content_imgs_dir\": cp[\"data_dir\"],\n", "        \"content_imgs_weights\": [1.0],\n", "        \"content_layers\": ['conv4_2'], # ['conv4_2']\n", "        \"content_layer_weights\": [1e4],\n", "        # \"content_layers_weights\": [1e4], \n", "        # \"content_layers_weights\": [1e2], \n", "        \"content_layers_weights\": [2.5 * 1e-8], \n", "        \"content_layer_weights\": [1.0],\n", "        \"content_loss_function\": 1,\n", "        \"content_weight\": 5e0,\n", "        \"frame_content_frmt\": 'frame{}.jpg',\n", "        \"content_imgs_weights\": [1.0],\n", "        \"content_layers_weights\": [2.5e-08],\n", "        \"content_loss_function\": 1,\n", "        \"content_weight\": 5e0,\n", "        \"content_weights_frmt\": 'reliable_{}_{}.txt',\n", "        \"style_imgs_dir\": cp[\"data_dir\"],\n", "        \"style_scale\": 1,\n", "        \"style_imgs_weights\": [1.0],\n", "        # \"style_layers\": ['relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1'],\n", "        \"style_layers\": ['conv1_1','conv2_1','conv3_1','conv4_1','conv5_1'],\n", "        # \"style_layers_weights\": [1e-2],\n", "        # \"style_layers_weights\": [1e2],  \n", "        # \"style_layers_weights\": [1e-6 / 100],  \n", "        # \"style_layers_weights\": [1e-2, 1e-2, 1e-2, 1e-2, 1e-2],\n", "        \"style_layers_weights\": [0.2, 0.2, 0.2, 0.2, 0.2],\n", "        \"style_weight\": 1e4,\n", "        \"style_imgs_weights\": [1.0],\n", "        \"style_layers_weights\": [0.01, 0.01, 0.01, 0.01, 0.01],\n", "        \"style_layers_weights\": [0.2, 0.2, 0.2, 0.2, 0.2],\n", "        \"style_weight\": 1e4,\n", "        \"style_mask\": 0,\n", "        \"max_size\": 512,\n", "        \"input_shape\": (224, 224, 3), # (512, 512, 3) # \n", "        \"frame_first_iterations\": 2000,\n", "        \"frame_first_type\": 'content', # args.frame_first_type ['random', 'content', 'style'\n", "        \"init_image_type\": \"content\", # 'content','style','init','random','prev','prev_warped'\n", "        \"frame_init_type\": 'prev',\n", "        \"init_img_dir\": cp[\"data_dir\"],\n", "        \"init_image_type\": \"content\",\n", "        \"frame_first_iterations\": 2000,\n", "        \"init_img_type\": 'content',\n", "        \"img_name\": 'result',\n", "        \"original_colors\": None,\n", "        \"color_convert_type\": 'yuv',\n", "        \"color_convert_time\": 'after',\n", "        \"optimizer\": 'adam', # 'lbfgs' # \n", "        \n", "        # \"total_variation_weight\": 1e-6, # 300,\n", "        \"total_variation_weight\": 0.001,\n", "        \"total_variation_weight\": 1e-3,\n\n", "        # \"video_input_dir\": os.path.join(cp[\"proj_dir\"], 'intput_vid'),        \n", "        \"video_input_dir\": os.path.join(cp[\"data_dir\"], ''),\n", "        \"video_output_dir\": os.path.join(cp[\"results_dir\"], 'output_vid'),\n", "        \"video_output_dir\": os.path.join(cp[\"data_dir\"], 'output_vid'),\n", "        \"video_dir\": os.path.join(cp[\"lab\"], \"\"),\n", "        \"video_frames_dir\": os.path.join(cp[\"lab\"], \"NewYork\"),\n", "        \"video_frames_dir\": os.path.join(cp[\"results_dir\"], 'frames'),\n", "        \"video_output_dir\": os.path.join(cp[\"data_dir\"], 'output_vid'),\n", "        \"video_file\": 'videoframe.mp4',\n", "        \"video_file\": 'Streets of New York City 4K video-vCdBIRtsL6o.f313.mp4',\n", "        \"video_file\": 'portu.mp4',\n", "        # \"video_input_dir\": os.path.join(cp[\"data_dir\"], 'intput_vid'),\n", "        \"video_output_dir\": os.path.join(cp[\"results_dir\"], 'output_vid'),\n", "        \"video_dir\": os.path.join(cp[\"lab\"], \"\"),\n", "        # \"video_path\": os.path.join(video_dir, video_file),\n", "        \"backward_optical_flow_frmt\": 'backward_{}_{}.flo',\n", "        \"forward_optical_flow_frmt\": 'forward_{}_{}.flo',\n", "        \"video\": 0,\n", "        \"frames_dir\": os.path.join(cp[\"results_dir\"], 'cromes/frames'),\n", "        \"frame_iterations\": 800,\n", "        \"frame_end\": 9,\n", "        \"frame\": None,    \n", "        \"frame_start\": 0,\n", "        \"frame_end\": -1, # num_frames\n", "        \"frame_first_iterations\": 20, # 2000\n", "        \"frame_init_type\": 'prev', # 'prev_warped'\n", "        \"frame_content_frmt\": 'frame{}.jpg',\n", "        \"frame_first_type\": 'content',\n", "        \"temporal_weight\": 2e2,\n", "        \"img_output_dir\": cp[\"results_dir\"],\n", "        \"image_step_format\": 'step{}.jpg',\n", "        \"image_epoch_format\": 'epoch{}.jpg',\n", "        \"show_entry_imgs\": 0,\n", "        \"show_set_imgs\": 1,\n", "        \"show_fit_imgs\": 1,\n", "        \"zfill\": 4,\n", "        \"device\": '/gpu:0',\n", "    }\n", "    xp={}\n", "    for key in cp.keys():\n", "        xp[key] = cp[key]\n", "    tree = ontree.tree(cp)\n", "    for key in tree.keys():\n", "        xp[key] = tree[key]\n", "    for key in yp.keys():\n", "        print(key, yp[key])\n", "        xp[key] = yp[key]\n", "   \n", "    return xp\n", "#\n", "#\n", "#   FUNS VIDEO\n", "#\n", "#\n", "def get_content_frame(frame, args):\n", "    video_input_dir = args.video_input_dir\n", "    frame_content_frmt = args.frame_content_frmt\n\n", "    # frame_content_frmt = 'frame_{}.ppm' # args.frame_content_frmt\n", "    zfill = 4 # args.zfill\n", "    frame_name = frame_content_frmt.format(str(frame).zfill(zfill))\n", "    # path_to_img = os.path.join(video_input_dir, frame_name)\n\n", "    # print(\"get_content_frame path\", path_to_img)    \n", "    # img = read_image(path_to_img)\n", "    path = os.path.join(args.video_input_dir, frame_name)\n", "    img = onfile.path_to_tnua_with_tf(path, args)\n", "    return img\n", "#\n", "def _get_style_images(content_img, args=None):\n", "  _, ch, cw, cd = content_img.shape\n", "  style_imgs = []\n", "  for style_fn in args.style_imgs_files:\n", "    path = os.path.join(args.style_imgs_dir, style_fn)\n", "    # bgr image\n", "    img = cv2.imread(path, cv2.IMREAD_COLOR)\n", "    img = img.astype(np.float32)\n", "    img = cv2.resize(img, dsize=(cw, ch), interpolation=cv2.INTER_AREA)\n", "    img = onvgg.vgg_preprocess(img)\n", "    style_imgs.append(img)\n", "  return style_imgs\n", "#\n", "def cv2_frame(cvi, refimg, scale):\n", "    _, ch, cw, cd = refimg.shape #  (1, 256, 256, 3) [[[[-107.68   -90.779  -78.939]\n", "    img = cvi\n", "    img = img.astype(np.float32) # shape: (657, 1000, 3) [[[ 51.  68. 135.]\n", "    sh, sw, sd = img.shape # shape: (657, 1000, 3)\n\n", "    # use scale args to resize and tile image\n", "    scaled_img = cv2.resize(img, dsize=(int(sw*scale), int(sh*scale)), \n", "        interpolation=cv2.INTER_AREA)\n", "    ssh, ssw, ssd = scaled_img.shape # shape: (657, 1000, 3)\n", "    if ssh > ch and ssw > cw:\n", "        starty = int((ssh-ch)/2)\n", "        startx = int((ssw-cw)/2)\n", "        img = scaled_img[starty:starty+ch, startx:startx+cw]\n", "    elif ssh > ch:\n", "        starty = int((ssh-ch)/2)\n", "        img = scaled_img[starty:starty+ch, 0:ssw]\n", "    # if ssw != cw:   # scaled_style_width != content_width 1000 != 256 \n", "    if ssw <= cw:   # scaled_style_width != content_width 1000 != 256 \n", "        # cv2.copyMakeBorder(src, top, bottom, left, right, borderType, value)\n", "        img = cv2.copyMakeBorder(img,0,0,0,(cw-ssw),cv2.BORDER_REFLECT)\n", "    elif ssw > cw:\n", "        startx = int((ssw-cw)/2)\n", "        img = scaled_img[0:ssh, startx:startx+cw]\n", "    # if ssh != ch:\n", "    if ssh <= ch:\n", "        img = cv2.copyMakeBorder(img,0,(ch-ssh),0,0,cv2.BORDER_REFLECT)\n", "    # else:\n", "    if ssh <= ch and ssw <= cw:\n", "        img = cv2.copyMakeBorder(scaled_img,0,(ch-ssh),0,(cw-ssw),cv2.BORDER_REFLECT)\n", "    return img\n", "#\n", "def get_style_cvis(content_img, args):\n", "    if args.verbose >0:\n", "        print(f'|---> get_style_cvis  \\n \\\n", "            content_img: {args.style_imgs_files} \\n \\\n", "            args.style_imgs_dir: {args.style_imgs_dir} \\n \\\n", "        ')\n", "    style_imgs=[]\n", "    style_imgs_dir=args.style_imgs_dir\n", "    max_size=args.max_size\n", "    style_scale =args.style_scale\n", "    _, ch, cw, cd = content_img.shape #  (1, 256, 256, 3) [[[[-107.68   -90.779  -78.939]\n", "    mx = max_size\n", "    cvis = []\n", "    for style_fn in args.style_imgs_files:\n", "        path = os.path.join(style_imgs_dir, style_fn)\n", "        if args.verbose >0:\n", "            print(f'|...> get_style_cvis path {path}')        \n", "        # bgr image\n", "        cvi = cv2.imread(path, cv2.IMREAD_COLOR)\n", "        img = cv2_frame(cvi, content_img, style_scale)\n\n", "        # img = onvgg.vgg_preprocess(img)\n", "        cvis.append(img)\n", "    return cvis\n", "#\n", "def get_input_image(\n", "        init_type= 'content', # {content,style,init,random,prev,prev_warped}\n", "        content_img=None, \n", "        style_imgs=[], \n", "        init_img=None, \n", "        frame=None, \n", "        video_input_dir='./', \n", "        video_output_dir='./', \n", "        frame_content_frmt = 'frame_{}.ppm',\n", "        zill=4,\n", "        noise_ratio = 1.0, \n", "        args=None\n", "    ):\n", "    if args.verbose >0:\n", "        print(f\"|---> get_input_image of type: {init_type} with frame: {frame}\")\n", "    if init_type == 'content':\n", "        return content_img\n", "    elif init_type == 'style':\n", "        style_img = style_imgs[0]   # _e_\n", "        return style_img\n", "    elif init_type == 'init':\n", "        return init_img\n", "    elif init_type == 'random':\n", "        init_img = get_noise_image(noise_ratio, content_img)\n", "        return init_img\n", "    # only for video frames\n", "    elif init_type == 'prev':\n", "        assert(not frame == None)\n", "        #init_img = get_prev_frame(\n", "        #    frame, \n", "        #    video_output_dir,\n", "        #    frame_content_frmt,\n", "        #    zill,\n", "        #)\n", "        init_img = get_prev_frame(\n", "            frame, \n", "            args\n", "        )        \n", "        return init_img\n", "    elif init_type == 'prev_warped':\n", "        #init_img = get_prev_warped_frame(\n", "        #    frame, \n", "        #    video_input_dir\n", "        #)\n", "        init_img = get_prev_warped_frame(\n", "            frame, \n", "            args\n", "        )        \n", "        return init_img\n", "#\n", "def get_optimizer(loss):\n", "    print_iterations = 100 # \n", "    max_iterations = 1000 # args.max_iterations\n", "    verbose = True # args.verbose\n", "    learning_rate = 1e0 # args.learning_rate\n", "    optimizer = 'adam' # args.optimizer\n", "    print_iterations = print_iterations if verbose else 0\n", "    if optimizer == 'lbfgs':\n", "        optimizer = tf.contrib.opt.ScipyOptimizerInterface(\n", "        loss, method='L-BFGS-B',\n", "        options={'maxiter': max_iterations,\n", "                    'disp': print_iterations})\n", "    elif optimizer == 'adam':\n", "        # optimizer = tf.train.AdamOptimizer(learning_rate)\n", "        optimizer = tf.optimizers.Adam(learning_rate)\n", "    return optimizer\n", "#\n", "def get_noise_image(noise_ratio, content_img, \n", "        seed = 0\n", "    ):\n", "    np.random.seed(seed)\n", "    noise_img = np.random.uniform(-20., 20., content_img.shape).astype(np.float32)\n", "    img = noise_ratio * noise_img + (1.-noise_ratio) * content_img\n", "    return img\n", "#\n", "def get_mask_image(mask_img, width, height, \n", "        content_imgs_dir = './',\n", "        args = None\n", "    ):\n", "    path = os.path.join(content_imgs_dir, mask_img)\n", "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n", "    #check_file(img, path)\n", "    if args.verbose > 0:\n", "        print(f'|---> get_mask_image {path}')\n", "    img = cv2.resize(img, dsize=(width, height), interpolation=cv2.INTER_AREA)\n", "    img = img.astype(np.float32)\n", "    mx = np.amax(img)\n", "    img /= mx\n", "    return img\n", "#\n", "def get_prev_frame(frame, args=None):\n", "    #   get_prev_frame: previously stylized frame    \n", "    video_output_dir = args.video_input_dir # _e_ tbc\n", "    frame_content_frmt = args.frame_content_frmt\n", "    zfill = args.zfill\n", "    \n", "    prev_frame = frame - 1\n", "    fn = frame_content_frmt.format(str(prev_frame).zfill(zfill))\n", "    path = os.path.join(video_output_dir, fn)\n", "    if args.verbose > 0:\n", "        print(f\"|---> get_prev_frame for frame: {frame} from {path}\")\n\n", "    # img = cv2.imread(path, cv2.IMREAD_COLOR)\n", "    # check_file(img, path)\n", "    img = onfile.path_cv_pil(path)\n", "    img = onformat.pil_to_dnua(img)\n", "    return img\n", "#\n", "def get_prev_warped_frame(frame, args=None):\n", "    video_input_dir = args.video_input_dir\n", "    backward_optical_flow_frmt = args.backward_optical_flow_frmt\n", "    frame_content_frmt = args.frame_content_frmt\n", "    prev_img = get_prev_frame(frame, args)\n", "    if args.verbose > 0:\n", "        print(f\"|---> get_prev_frame in {video_input_dir} with {frame_content_frmt}\")\n", "    prev_frame = frame - 1\n", "    # backwards flow: current frame -> previous frame\n", "    fn = backward_optical_flow_frmt.format(str(frame), str(prev_frame))\n", "    path = os.path.join(video_input_dir, fn)\n", "    flow = read_flow_file(path)\n", "    warped_img = warp_image(prev_img, flow).astype(np.float32)\n", "    img = onvgg.vgg_preprocess(warped_img)\n", "    return img\n", "#\n", "def get_content_weights(frame, prev_frame, args=None):\n", "    video_input_dir=args.video_input_dir\n", "    content_weights_frmt = args.content_weights_frmt\n", "    forward_fn = content_weights_frmt.format(str(prev_frame), str(frame))\n", "    backward_fn = content_weights_frmt.format(str(frame), str(prev_frame))\n", "    forward_path = os.path.join(video_input_dir, forward_fn)\n", "    backward_path = os.path.join(video_input_dir, backward_fn)\n", "    forward_weights = read_weights_file(forward_path)\n", "    backward_weights = read_weights_file(backward_path)\n", "    return forward_weights #, backward_weights\n", "#\n", "def warp_image(src, flow):\n", "    _, h, w = flow.shape\n", "    flow_map = np.zeros(flow.shape, dtype=np.float32)\n", "    for y in range(h):\n", "        flow_map[1,y,:] = float(y) + flow[1,y,:]\n", "    for x in range(w):\n", "        flow_map[0,:,x] = float(x) + flow[0,:,x]\n", "    # remap pixels to optical flow\n", "    dst = cv2.remap(\n", "        src, flow_map[0], flow_map[1], \n", "        interpolation=cv2.INTER_CUBIC, borderMode=cv2.BORDER_TRANSPARENT)\n", "    return dst\n", "#\n", "def convert_to_original_colors(content_img, stylized_img, args=None):\n", "    color_convert_type = args.color_convert_type\n", "    content_img  = onvgg.vgg_deprocess(content_img)\n", "    stylized_img = onvgg.vgg_deprocess(stylized_img)\n", "    if color_convert_type == 'yuv':\n", "        cvt_type = cv2.COLOR_BGR2YUV\n", "        inv_cvt_type = cv2.COLOR_YUV2BGR\n", "    elif color_convert_type == 'ycrcb':\n", "        cvt_type = cv2.COLOR_BGR2YCR_CB\n", "        inv_cvt_type = cv2.COLOR_YCR_CB2BGR\n", "    elif color_convert_type == 'luv':\n", "        cvt_type = cv2.COLOR_BGR2LUV\n", "        inv_cvt_type = cv2.COLOR_LUV2BGR\n", "    elif color_convert_type == 'lab':\n", "        cvt_type = cv2.COLOR_BGR2LAB\n", "        inv_cvt_type = cv2.COLOR_LAB2BGR\n", "    content_cvt = cv2.cvtColor(content_img, cvt_type)\n", "    stylized_cvt = cv2.cvtColor(stylized_img, cvt_type)\n", "    c1, _, _ = cv2.split(stylized_cvt)\n", "    _, c2, c3 = cv2.split(content_cvt)\n", "    merged = cv2.merge((c1, c2, c3))\n", "    dst = cv2.cvtColor(merged, inv_cvt_type).astype(np.float32)\n", "    dst = onvgg.vgg_preprocess(dst)\n", "    return dst\n", "#\n", "#\n", "#   FUNCS LOG\n", "#\n", "def write_image_output(output_img, \n", "        content_img, style_imgs, input_img, \n", "        epoch=0,\n", "        args=None,\n", "    ):\n", "    if args.verbose > 0:\n", "        print(f'|---> write_image_output \\n \\\n", "            to img_output_dir: {args.img_output_dir} \\n \\\n", "            epoch: {epoch} \\n \\\n", "        ')\n", "    output_img = onformat.nua_to_pil(output_img)\n", "    content_img = onformat.nua_to_pil(content_img)\n", "    style_imgs = onformat.dnuas_to_pils(style_imgs)\n", "    input_img = onformat.nua_to_pil(input_img)\n\n", "    # save result image\n", "    img_file = f'output_'+str(epoch).zfill(args.zfill)+'.png'\n", "    img_path = os.path.join(args.img_output_dir, img_file)\n", "    onfile.pil_to_file_with_cv(img_path, output_img)\n\n", "    # save content image\n", "    content_file = f'content.jpg'\n", "    content_path = os.path.join(args.img_output_dir, content_file)\n", "    onfile.pil_to_file_with_cv(content_path, content_img)\n\n", "    # save input image\n", "    init_file = f'init.jpg'\n", "    init_path = os.path.join(args.img_output_dir, init_file)\n", "    onfile.pil_to_file_with_cv(init_path, input_img)\n", "    style_files = []\n", "    style_paths = []\n\n", "    # save style images\n", "    for i, style_img in enumerate(style_imgs):\n", "        style_files.append(f'style_{str(i)}.png')\n", "        style_paths.append(os.path.join(args.img_output_dir, style_files[i]))\n", "        path = style_paths[i]\n", "        onfile.pil_to_file_with_cv(path, style_img)\n\n", "    # save the configuration settings\n", "    out_file = os.path.join(args.img_output_dir, 'meta_data.txt')\n", "    f = open(out_file, 'w')\n", "    f.write(f'image_name: {img_path}\\n')\n", "    f.write(f'content: {content_path}\\n')\n", "    index = 0\n", "    for i, style_path in enumerate(style_paths):\n", "        f.write(f'styles[{str(i)}]: {style_path}\\n')\n", "    if 'mask_imgs_files' in vars(args).keys():\n", "        for i, style_mask_img in enumerate(args.mask_imgs_files):\n", "            style_mask_img_path = os.path(args.style_imgs_dir, args.mask_imgs_files[i])\n", "            f.write(f'style_masks[{str(i)}]: {style_mask_img_path}\\n')        \n", "    f.write(f'init_type: {args.init_img_type}\\n')\n", "    f.write(f'content_weight: {args.content_weight}\\n')\n", "    f.write(f'content_layers: {args.content_layers}\\n')\n", "    f.write(f'content_layers_weights: {args.content_layers_weights}\\n')\n", "    f.write(f'style_imgs_weights: {args.style_imgs_weights}\\n')\n", "    f.write(f'style_layers: {args.style_layers}\\n')\n", "    f.write(f'style_layers_weights: {args.style_layers_weights}\\n')\n", "    f.write(f'total_variation_weight: {args.total_variation_weight}\\n')\n", "    f.write(f'content_size: {args.content_size}\\n')\n", "    f.write(f'input_shape: {args.input_shape}\\n')\n", "    f.write(f'optimizer: {args.optimizer}\\n')\n", "    f.write(f'max_size: {args.max_size}\\n')\n", "    f.write(f'frame_iterations: {args.frame_iterations}\\n')\n", "    f.write(f'max_epochs: {args.max_epochs}\\n')\n", "    f.write(f'steps_per_epoch: {args.steps_per_epoch}\\n')\n", "    f.write(f'print_iterations: {args.print_iterations}\\n')\n", "    f.close()\n", "#\n", "#\n", "#   FUNCS MODEL\n", "#\n", "def read_weights_file(path):\n", "    lines = open(path).readlines()\n", "    header = list(map(int, lines[0].split(' ')))\n", "    w = header[0]\n", "    h = header[1]\n", "    vals = np.zeros((h, w), dtype=np.float32)\n", "    for i in range(1, len(lines)):\n", "        line = lines[i].rstrip().split(' ')\n", "        vals[i-1] = np.array(list(map(np.float32, line)))\n", "        vals[i-1] = list(map(lambda x: 0. if x < 255. else 1., vals[i-1]))\n", "    # expand to 3 channels\n", "    weights = np.dstack([vals.astype(np.float32)] * 3)\n", "    return weights\n", "#\n", "def get_content_weights(frame, prev_frame):\n", "    content_weights_frmt = 'reliable_{}_{}.txt' # args.content_weights_frmt\n", "    video_input_dir = './video_input' # args.video_input_dir\n", "    forward_fn = content_weights_frmt.format(str(prev_frame), str(frame))\n", "    backward_fn = content_weights_frmt.format(str(frame), str(prev_frame))\n", "    forward_path = os.path.join(video_input_dir, forward_fn)\n", "    backward_path = os.path.join(video_input_dir, backward_fn)\n", "    forward_weights = read_weights_file(forward_path)\n", "    backward_weights = read_weights_file(backward_path)\n", "    return forward_weights #, backward_weights\n", "#\n", "def minimize_with_lbfgs(sess, net, optimizer, init_img,\n", "        verbose=True,\n", "    ):\n", "    if verbose: print('\\nMINIMIZING LOSS USING: L-BFGS OPTIMIZER')\n", "    init_op = tf.global_variables_initializer()\n", "    sess.run(init_op)\n", "    sess.run(net['input'].assign(init_img))\n", "    optimizer.minimize(sess)\n", "#\n", "def minimize_with_adam(sess, net, optimizer, init_img, loss,\n", "        verbose=True,\n", "        max_iterations=1000,\n", "        print_iterations=50,\n", "    ):\n", "    if verbose: print('\\nMINIMIZING LOSS USING: ADAM OPTIMIZER')\n", "    train_op = optimizer.minimize(loss)\n", "    init_op = tf.global_variables_initializer()\n", "    sess.run(init_op)\n", "    sess.run(net['input'].assign(init_img))\n", "    iterations = 0\n", "    while (iterations < max_iterations):\n", "        sess.run(train_op)\n", "        if iterations % print_iterations == 0 and verbose:\n", "            curr_loss = loss.eval()\n", "            print(\"At iterate {}\\tf=  {}\".format(iterations, curr_loss))\n", "        iterations += 1\n", "#\n", "#\n", "#   NETS\n", "#\n", "# a neural algorithm for artistic style' loss functions\n", "def content_layer_loss(p, x, args=None):\n", "    _, h, w, d = p.get_shape()\n", "    M = h * w # h.value * w.value\n", "    N = d # d.value\n", "    if args.content_loss_function   == 1:\n", "        K = 1. / (2. * N**0.5 * M**0.5)\n", "    elif args.content_loss_function == 2:\n", "        K = 1. / (N * M)\n", "    elif args.content_loss_function == 3:  \n", "        K = 1. / 2.\n", "    loss = K * tf.reduce_sum(input_tensor=tf.pow((x - p), 2))\n", "    return loss\n", "#\n", "def style_layer_loss(a, x):\n", "    if 0:\n", "        print(f'|---> style_layer_loss')      \n", "    _, h, w, d = a.get_shape()\n", "    M = h * w # h.value * w.value\n", "    N = d # d.value\n", "    A = gram_matrix(a, M, N)\n", "    G = gram_matrix(x, M, N)\n", "    loss = (1./(4 * N**2 * M**2)) * tf.reduce_sum(input_tensor=tf.pow((G - A), 2))\n", "    return loss\n", "#\n", "def gram_matrix(x, area, depath):\n", "    F = tf.reshape(x, (area, depath))\n", "    G = tf.matmul(tf.transpose(a=F), F)\n", "    return G\n", "#\n", "def mask_style_layer(a, x, mask_img, args=None):\n", "    if args.verbose > 0:\n", "        print(f'|---> mask_style_layer {mask_img}')    \n", "    _, h, w, d = a.get_shape()\n", "    #mask = get_mask_image(mask_img, w.value, h.value)\n", "    mask = get_mask_image(mask_img, w, h,\n", "        content_imgs_dir=args.style_imgs_dir,\n", "        args=args) # _e_\n", "    mask = tf.convert_to_tensor(value=mask)\n", "    tensors = []\n", "    # for _ in range(d.value): \n", "    for _ in range(d): \n", "        tensors.append(mask)\n", "    mask = tf.stack(tensors, axis=2)\n", "    mask = tf.stack(mask, axis=0)\n", "    mask = tf.expand_dims(mask, 0)\n", "    a = tf.multiply(a, mask)\n", "    x = tf.multiply(x, mask)\n", "    return a, x\n", "#\n", "def sum_masked_style_losses(net, combo, style_imgs, args=None):\n", "    if args.verbose > 0:\n", "        print(f'|---> sum_masked_style_losses')\n", "    total_style_loss = 0.\n", "    weights = args.style_imgs_weights\n", "    masks = args.style_mask_imgs\n", "    for img, img_weight, img_mask in zip(style_imgs, weights, masks):\n", "        #net['input'].assign(img)\n", "        ps = net.extract_features([combo], args.style_layers)[0]   # net[layer]\n", "        xs = net.extract_features([img], args.style_layers)[0]   # net[layer]          \n", "        style_loss = 0.\n", "        for layer, weight in zip(args.style_layers, args.style_layers_weights):\n", "            a = ps[layer] # net[layer]\n", "            x = xs[layer] # net[layer]\n", "            #a = tf.convert_to_tensor(value=a)\n", "            a, x = mask_style_layer(a, x, img_mask, args)\n", "            style_loss += style_layer_loss(a, x) * weight\n", "        style_loss /= float(len(args.style_layers))\n", "        total_style_loss += (style_loss * img_weight)\n", "    total_style_loss /= float(len(style_imgs))\n", "    return total_style_loss\n", "#\n", "def sum_style_losses(net, combo, style_imgs, args=None):\n", "    if args.verbose > 0:\n", "        print(f'|---> sum_style_losses')    \n", "    total_style_loss = 0.\n", "    weights = args.style_imgs_weights\n", "    for img, img_weight in zip(style_imgs, weights):\n", "            # net['input'].assign(img)\n", "            ps = net.extract_features([combo], args.style_layers)[0]   # net[layer]\n", "            xs = net.extract_features([img], args.style_layers)[0]   # net[layer]            \n", "            style_loss = 0.\n", "            for layer, weight in zip(args.style_layers, args.style_layers_weights):\n", "                p = ps[layer] # net[layer]\n", "                x = xs[layer] # net[layer]\n", "                # p = tf.convert_to_tensor(value=p)\n", "                style_loss += style_layer_loss(p, x) * weight\n", "            style_loss /= float(len(args.style_layers))\n", "            total_style_loss += (style_loss * img_weight)\n", "    total_style_loss /= float(len(style_imgs))\n", "    return total_style_loss\n", "#\n", "def sum_content_losses(net, combo, content_img, args=None):\n", "    if v\n", "        print(f'|---> sum_content_losses')     \n", "    # net['input'].assign(content_img)\n", "    content_loss = 0.\n", "    ps = net.extract_features([combo], args.content_layers)[0]   # net[layer]\n", "    xs = net.extract_features([content_img], args.content_layers)[0]   # net[layer]\n", "    for layer, weight in zip(args.content_layers, args.content_layer_weights):\n", "            p = ps[layer]   # net[layer]\n", "            x = xs[layer]   # net[layer]\n", "            # p = tf.convert_to_tensor(value=p)\n", "            content_loss += content_layer_loss(p, x, args) * weight\n", "    content_loss /= float(len(args.content_layers))\n", "    return content_loss\n", "#\n", "# artistic style transfer for videos' loss functions\n", "def temporal_loss(x, w, c):\n", "  c = c[np.newaxis,:,:,:]\n", "  D = float(x.size)\n", "  loss = (1. / D) * tf.reduce_sum(input_tensor=c * tf.nn.l2_loss(x - w))\n", "  loss = tf.cast(loss, tf.float32)\n", "  return loss\n", "#\n", "def get_longterm_weights(i, j, args=None):\n", "  c_sum = 0.\n", "  for k in range(args.prev_frame_indices):\n", "    if i - k > i - j:\n", "      c_sum += get_content_weights(i, i - k, args)\n", "  c = get_content_weights(i, i - j, args)\n", "  c_max = tf.maximum(c - c_sum, 0.)\n", "  return c_max\n", "#\n", "def sum_longterm_temporal_losses(sess, net, frame, input_img, args=None):\n", "  x = sess.run(net['input'].assign(input_img))\n", "  loss = 0.\n", "  for j in range(args.prev_frame_indices):\n", "    prev_frame = frame - j\n", "    w = input_img # _e_\n", "    if args.frame_init_type == 'prev':\n", "        w = get_prev_frame(frame, args) # _e_\n", "    elif args.frame_init_type == 'prev_warped':\n", "        w = get_prev_warped_frame(frame, args)\n", "    c = get_longterm_weights(frame, prev_frame)\n", "    loss += temporal_loss(x, w, c)\n", "  return loss\n", "#\n", "def sum_shortterm_temporal_losses(sess, net, frame, input_img, args=None):\n", "  x = sess.run(net['input'].assign(input_img))\n", "  prev_frame = frame - 1\n", "  w = input_img # _e_\n", "  if args.frame_init_type == 'prev':\n", "      w = get_prev_frame(frame, args) # _e_\n", "  elif args.frame_init_type == 'prev_warped':\n", "      w = get_prev_warped_frame(frame, args)\n", "  c = get_content_weights(frame, prev_frame, args) # _e_\n", "  loss = temporal_loss(x, w, c)\n", "  return loss\n", "#\n", "# utilities and i/o\n", "def read_image(path):\n", "  # bgr image\n", "  img = cv2.imread(path, cv2.IMREAD_COLOR)\n", "  check_image(img, path)\n", "  img = img.astype(np.float32)\n", "  img = onvgg.vgg_preprocess(img)\n", "  return img\n", "#\n", "def write_image(path, img):\n", "  img = onvgg.vgg_deprocess(img)\n", "  cv2.imwrite(path, img)\n", "#\n", "def preprocess_tensor(inputs):\n", "    # Keras works with batches of images. \n", "    # So, the first dimension is used for the number of samples (or images) you have.\n", "    # vgg_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32).reshape((3,1,1))\n", "    VGG_BGR_MEAN = [103.939, 116.779, 123.68]\n", "    vgg_bgr_mean_arr = np.array(VGG_BGR_MEAN)\n", "    imgpre = inputs[0] # np.squeeze(inputs, axis = 0) # \n", "    imgpre = imgpre[...,::-1] # BGR => RGB\n", "    imgpre = imgpre[np.newaxis,:,:,:] # shape (h, w, d) to (1, h, w, d)\n", "    imgpre -= vgg_bgr_mean_arr.reshape((1,1,1,3))\n", "    imgpre = tf.convert_to_tensor(imgpre)\n", "    return imgpre\n", "#\n", "def preprocess_tensors(inputs):\n", "    pretensors = []\n", "    for item in inputs:\n", "        preitem = preprocess_tensor(item)\n", "        pretensors.append(preitem)\n", "    return pretensors\n", "#\n", "def show_effect_a(path):\n", "    from PIL import Image\n", "    image = Image.open(path)\n", "    image.show()\n", "    import cv2\n", "    img = cv2.imread(path, cv2.IMREAD_COLOR)\n", "    if 0:\n", "        cv2.imshow('img', img)\n", "        cv2.waitKey(0)      \n", "    img = onvgg.vgg_preprocess(img) # <class 'numpy.ndarray'> (1, 1654, 1654, 3)\n", "    print(type(img))\n", "    print(np.shape(img))\n", "    rgb = tf.squeeze(img, axis=0)\n", "    rgb = np.array(rgb, dtype=np.uint8)\n", "    rgb = PIL.Image.fromarray(rgb)\n", "    rgb.show()\n", "    img = onvgg.vgg_deprocess(img)\n", "    image = Image.fromarray(img)\n", "    image.show() \n", "#\n", "def read_flow_file(path):\n", "  with open(path, 'rb') as f:\n", "    # 4 bytes header\n", "    header = struct.unpack('4s', f.read(4))[0]\n", "    # 4 bytes width, height    \n", "    w = struct.unpack('i', f.read(4))[0]\n", "    h = struct.unpack('i', f.read(4))[0]   \n", "    flow = np.ndarray((2, h, w), dtype=np.float32)\n", "    for y in range(h):\n", "      for x in range(w):\n", "        flow[0,y,x] = struct.unpack('f', f.read(4))[0]\n", "        flow[1,y,x] = struct.unpack('f', f.read(4))[0]\n", "  return flow\n", "#\n", "def read_weights_file(path):\n", "  lines = open(path).readlines()\n", "  header = list(map(int, lines[0].split(' ')))\n", "  w = header[0]\n", "  h = header[1]\n", "  vals = np.zeros((h, w), dtype=np.float32)\n", "  for i in range(1, len(lines)):\n", "    line = lines[i].rstrip().split(' ')\n", "    vals[i-1] = np.array(list(map(np.float32, line)))\n", "    vals[i-1] = list(map(lambda x: 0. if x < 255. else 1., vals[i-1]))\n", "  # expand to 3 channels\n", "  weights = np.dstack([vals.astype(np.float32)] * 3)\n", "  return weights\n", "#\n", "def normalize(weights):\n", "    denom = sum(weights)\n", "    if denom > 0.:\n", "        return [float(i) / denom for i in weights]\n", "    else: \n", "        return [0.] * len(weights)\n", "#\n", "def check_image(img, path):\n", "    if img is None:\n", "        raise OSError(errno.ENOENT, \"No such file\", path)\n", "#\n", "# rendering -- where the magic happens\n", "def compute_loss(combo):\n", "    loss = tf.zeros(shape=())\n", "#\n", "def clip_0_1(image):\n", "    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n", "#\n", "def build_vgg(\n", "        input_shape = (224, 224, 3),\n", "        model_classes = 1000,\n", "        model_pooling=None,\n", "        model_include_top = True,\n", "        model_weights = None,\n", "    ):\n", "    inputs = tf.keras.layers.Input(shape = input_shape)\n\n", "    # Block 1\n", "    x = tf.keras.layers.Conv2D(64, (3,3),activation='relu',padding='same',name='conv1_1')(inputs)\n", "    x = tf.keras.layers.Conv2D(64, (3,3),activation='relu',padding='same',name='conv1_2')(x)\n", "    x = tf.keras.layers.MaxPooling2D((2,2), strides=(2,2), name='pool1')(x)\n\n", "    # Block 2\n", "    x = tf.keras.layers.Conv2D(128, (3,3),activation='relu',padding='same',name='conv2_1')(x)\n", "    x = tf.keras.layers.Conv2D(128, (3,3),activation='relu',padding='same',name='conv2_2')(x)\n", "    x = tf.keras.layers.MaxPooling2D((2,2), strides=(2,2), name='pool2')(x)\n\n", "    # Block 3\n", "    x = tf.keras.layers.Conv2D(256, (3,3),activation='relu',padding='same',name='conv3_1')(x)\n", "    x = tf.keras.layers.Conv2D(256, (3,3),activation='relu',padding='same',name='conv3_2')(x)\n", "    x = tf.keras.layers.Conv2D(256, (3,3),activation='relu',padding='same',name='conv3_3')(x)\n", "    x = tf.keras.layers.Conv2D(256, (3,3),activation='relu',padding='same',name='conv3_4')(x)\n", "    x = tf.keras.layers.MaxPooling2D((2,2), strides=(2,2), name='pool3')(x)\n\n", "    # Block 4\n", "    x = tf.keras.layers.Conv2D(512, (3,3),activation='relu',padding='same',name='conv4_1')(x)\n", "    x = tf.keras.layers.Conv2D(512, (3,3),activation='relu',padding='same',name='conv4_2')(x)\n", "    x = tf.keras.layers.Conv2D(512, (3,3),activation='relu',padding='same',name='conv4_3')(x)\n", "    x = tf.keras.layers.Conv2D(512, (3,3),activation='relu',padding='same',name='conv4_4')(x)\n", "    x = tf.keras.layers.MaxPooling2D((2,2), strides=(2,2), name='pool4')(x)\n\n", "    # Block 5\n", "    x = tf.keras.layers.Conv2D(512, (3,3),activation='relu',padding='same',name='conv5_1')(x)\n", "    x = tf.keras.layers.Conv2D(512, (3,3),activation='relu',padding='same',name='conv5_2')(x)\n", "    x = tf.keras.layers.Conv2D(512, (3,3),activation='relu',padding='same',name='cpmv5_3')(x)\n", "    x = tf.keras.layers.Conv2D(512, (3,3),activation='relu',padding='same',name='conv5_4')(x)\n", "    x = tf.keras.layers.MaxPooling2D((2,2), strides=(2,2), name='pool5')(x)\n", "    if model_include_top:\n", "        print(\"include classification block: flatten, fc1, fc2, predictions\")\n", "        # Classification block\n", "        x = tf.keras.layers.Flatten(name='flatten')(x)\n", "        x = tf.keras.layers.Dense(4096, activation='relu', name='fc1')(x)\n", "        x = tf.keras.layers.Dense(4096, activation='relu', name='fc2')(x)\n", "        x = tf.keras.layers.Dense(model_classes, activation='softmax', name='predictions')(x)\n", "    else:\n", "        if model_pooling == 'avg':\n", "            x = tf.keras.layers.GlobalAveragePooling2D()(x)\n", "        elif model_pooling == 'max':\n", "            x = tf.keras.layers.GlobalMaxPooling2D()(x)\n", "    outputs = x\n", "    vgg = tf.keras.Model(inputs, outputs)\n", "    if model_weights:\n", "        print(\"|...> load model_weights\")\n", "        vgg.load_weights(model_weights)\n", "    else:\n", "        print(\"|...> did not load model_weights\")\n", "    return vgg\n", "#\n", "#\n", "class GAN(tf.keras.models.Model):\n", "    def __init__(self, \n", "        input_shape = (224, 224, 3),        \n", "        args=None, \n", "    ):\n", "        super(GAN, self).__init__()\n", "        content_layers = args.content_layers\n", "        style_layers = args.style_layers\n", "        model_weights = args.model_weights\n", "        model_include_top = args.model_include_top\n", "        model_classes = args.model_classes\n", "        model_pooling = args.model_pooling\n", "        \n", "        self.num_content_layers = len(content_layers)\n", "        self.num_style_layers = len(style_layers)\n\n", "        # shap = np.shape(tf.squeeze(content_img, axis=0))\n", "        self.imgshape = input_shape\n", "        vggmodel = build_vgg(\n", "            input_shape = self.imgshape,\n", "            model_classes = model_classes,\n", "            model_pooling = model_pooling,\n", "            model_include_top = model_include_top,\n", "            model_weights = model_weights,\n", "        )\n", "        layer_names = content_layers + style_layers\n", "        if args.verbose > 0:\n", "            print(\"|...> GAN content_layers\", content_layers)\n", "            print(\"|...> GAN style_layers\", style_layers)\n", "            print(\"|...> GAN layers\", layer_names)\n", "        outputs = {name: vggmodel.get_layer(name).output for name in layer_names}        \n", "        # outputs = [vggmodel.get_layer(name).output for name in layer_names]\n", "        self.net = tf.keras.Model([vggmodel.input], outputs) \n", "        self.net.trainable = False        \n", "        if args.verbose > 0:\n", "            self.net.summary()\n", "        self.optimizer = tf.optimizers.Adam(\n", "            learning_rate=0.01,\n", "            beta_1=0.99, \n", "            epsilon=1e-1\n", "        )\n\n", "        #self.optimizer = tf.keras.optimizers.SGD(\n", "        #    learning_rate=0.1, \n", "        #    momentum=0.9, \n", "        #    #nesterov=False, \n", "        #    #name='SGD'\n", "        #)    \n\n", "        # if not hasattr(self, \"image\"):  # Or set self.v to None in __init__ \n", "        #     image = tf.Variable(self.content_img) \n", "        # self.image = image\n", "    def extract_features(self, imgs, layers=None):\n", "        net = self.net\n", "        imgshape = self.imgshape\n", "        if len(imgshape) == 3:\n", "            imgshape = imgshape[:-1]\n", "        (width,height) = imgshape\n", "        rei = []\n", "        for i,sty_img in enumerate(imgs):\n", "            img = imgs[i] # _e_\n", "            preprocessed_input = onvgg.tnua_to_vgg(img, width=width, height=height)\n", "            if 0:\n", "                print(f'|===> extract_features \\n \\\n", "                    preprocessed_input {type(preprocessed_input)} {np.shape(preprocessed_input)} \\n \\\n", "                ')\n", "            outputs = net(preprocessed_input)\n", "            features = {}\n", "            if layers:\n", "                for layer in layers:\n", "                    features[layer]=outputs[layer]\n", "            else:\n", "                features = outputs\n", "            rei.append(features)\n", "        return rei            \n", "    def call(self, image = None):\n", "        print(f'|===> GAN call {type(image)}')\n", "        img = self.image # <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n", "        img = onformat.nua_to_pil(img)\n", "        img.show()\n", " \n\n", "    # @tf.function\n", "    def train_step(self, image, content_img, style_imgs, args = None):\n", "        net = self.net\n", "        total_variation_weight = args.total_variation_weight\n", "        style_weight = args.style_weight\n", "        content_weight = args.content_weight\n", "        video = args.video\n", "        frame = args.frame\n", "        temporal_weight = args.temporal_weight\n", "        content_layer_weight = args.content_layer_weights\n", "        style_imgs_weights = args.style_imgs_weights\n", "        style_layers_weight = args.style_layers_weights\n", "        with tf.GradientTape() as tape:\n", "           \n", "        #     # content loss\n", "            L_content = sum_content_losses(self, image, content_img, args)\n\n", "            # style loss\n", "            if args.style_mask:\n", "                L_style = sum_masked_style_losses(self, image, style_imgs, args)\n", "            else:\n", "                L_style = sum_style_losses(self, image, style_imgs, args)\n", "            L_tv = tf.image.total_variation(image)\n\n", "            # loss weights\n", "            alpha = args.content_weight\n", "            beta  = args.style_weight\n", "            theta = args.total_variation_weight\n", "            \n", "            # total loss\n", "            L_total  = 0.\n", "            L_total += alpha * L_content\n", "            L_total += beta  * L_style\n", "            L_total += theta * L_tv\n", "        grad = tape.gradient(L_total, image)\n", "        self.optimizer.apply_gradients([(grad, image)])\n", "        image.assign(clip_0_1(image))\n", "        self.image = image\n\n", "    #\n", "    def fit(self, input_img, content_img, style_imgs, \n", "            frame = None,\n", "            args  = None\n", "        ):\n", "        max_epochs = args.max_epochs\n", "        steps_per_epoch = args.steps_per_epoch\n", "        video = args.video\n", "        visual = args.visual\n", "        verbose = args.verbose\n", "        img_output_dir = args.img_output_dir\n", "        video_styled_dir = args.video_styled_dir\n", "        image_step_format = args.image_step_format\n", "        image_epoch_format = args.image_epoch_format\n", "        frame_content_frmt = args.frame_content_frmt\n", "        zfill = args.zfill\n", "        show_fit_imgs = args.show_fit_imgs\n", "        train_step = self.train_step\n", "        image = tf.Variable(input_img)   \n", "        self.image = image\n", "        print(f'|===> fit \\n \\\n", "            max_epochs: {max_epochs} \\n \\\n", "            steps_per_epoch: {steps_per_epoch} \\n \\\n", "            train_step: {train_step} \\n \\\n", "            input_img shape: {np.shape(input_img)} \\n \\\n", "            content_img shape: {np.shape(content_img)} \\n \\\n", "            style_imgs shape: {[np.shape(img) for img in style_imgs]} \\n \\\n", "            combo_img shape: {type(self.image)} {np.shape(self.image)} \\n \\\n", "            style_mask: {args.style_mask} \\n \\\n", "        ')\n", "        if visual > 1:\n", "            print(f'|---> pil input_img')\n", "            if 1: onplot.pil_show_nua(input_img)        \n", "            print(f'|---> pil content_img')\n", "            if 1: onplot.pil_show_nua(content_img)        \n", "            print(f'|---> pil style_imgs')\n", "            if 1: onplot.pil_show_nuas(style_imgs)   \n", "            print(f'|---> pil combo_img')\n", "            if 1: onplot.pil_show_nua(self.image)\n", "            if args.style_mask_imgs:\n", "                print(f'|---> pil combo_img')\n", "                onplot.pil_show_nua(self.image)\n\n", "        # # ============\n", "        start = time.time()\n\n", "        #if 0: print(f\"[   .   ] display input image {step} image\")\n", "        #img = onformat.nua_to_pil(image)\n", "        #img = ondata.pil_resize(img, (256,256))\n", "        #img.show()\n", "        for epoch in range(max_epochs):\n", "            step = 0\n", "            for step_in_epoch in range(steps_per_epoch):\n", "                step += 1\n", "                if 1 and step % args.print_iterations == 0:\n", "                    if video:\n", "                        print(f'|---> fit {frame}:{epoch}:{step}')\n", "                    else:\n", "                        print(f'|---> fit {epoch}:{step}')\n", "                # ==========================================\n", "                train_step(image, content_img, style_imgs, args)\n", "                # =========================================\n", "                print(\".\", end='')\n", "            if 1 and step % args.print_iterations == 0: # show results each n steps\n", "                if 0:\n", "                    try:\n", "                        # display image per epoch _e_\n", "                        display.clear_output(wait=True)\n", "                        print(f\"[   .   ] display in step {step} image\")\n", "                        display.display(onformat.nua_to_pil(self.image))\n", "                    except:\n", "                        print(\"could not display epoch images\")\n", "                else:\n", "                    print(f\"[   .   ] show epoch {epoch} image\")\n", "                    img = onformat.nua_to_pil(self.image)\n", "                    img.show()\n\n", "            # write image per epoch\n", "            img_name = image_epoch_format.format(str(epoch).zfill(zfill))        \n", "            img_path = os.path.join(img_output_dir, img_name)\n", "            #onfile.pil_to_file_with_cv(img_path, self.image)\n", "            if 1: # write image per epoch\n", "                write_image_output(\n", "                    output_img = self.image, \n", "                    content_img = content_img, \n", "                    style_imgs = style_imgs, \n", "                    input_img = input_img,\n", "                    epoch = epoch,\n", "                    args = args,\n", "                )\n", "            if visual > 1:\n", "                if 1:\n", "                    print(f'|---> pil combo_img epoch: {epoch}')\n", "                onplot.pil_show_nua(self.image)\n", "        if video:\n", "            frame_name = args.frame_content_frmt.format(str(frame).zfill(zfill))\n", "            outpath_path = os.path.join(video_styled_dir, frame_name) # styled\n", "            print(f\"|...> fit save frame to {outpath_path} on epoch {epoch}\")\n", "            output_img = onformat.nua_to_pil(image)\n", "            onfile.pil_to_file_with_cv(outpath_path, output_img)\n", "        end = time.time()\n", "        print(\"Total time: {:.1f}\".format(end-start))\n", "#\n", "#\n", "#   CMDS\n", "#\n", "#\n", "#\n", "#\n", "#   nnimg\n", "#\n", "#   stylize\n", "#    sum_style_losses\n", "#    sum_masked_style_losses args.style_mask\n", "def nnimg(args, kwargs):\n", "    if 0:\n", "        args.PROJECT = 'bomze' # https://github.com/tg-bomze/Style-Transfer-Collection\n", "        args.DATASET = 'bomze'\n", "        args.content_img_file = 'bomze-content.png'\n", "        args.content_size = (512, 512)        \n", "        args.init_img_name = 'bomze-content.png'\n", "        args.style_imgs_files = ['bomze-style.png']\n", "    if 0:\n", "        args.PROJECT = 'building'\n", "        args.DATASET = 'building'\n", "        args.content_img_file = 'IMG_4468.JPG'\n", "        args.content_size = (512, 512)        \n", "        args.init_img_name = 'IMG_4468.JPG'\n", "        args.style_imgs_files = ['EZNJVoTXsAgUh6M.jpg', 'EZsQWC1X0AQaxHs.jpg']\n", "    if 0:\n", "        args.PROJECT = 'labrador'\n", "        args.DATASET = 'labrador'        \n", "        args.content_img_file = 'YellowLabradorLooking_new.jpg'\n", "        args.content_size = (512, 512)        \n", "        args.init_img_name = 'YellowLabradorLooking_new.jpg'\n", "        args.style_imgs_files = ['starry-night.jpg']\n", "    if 0:\n", "        args.PROJECT = 'kandinsky'\n", "        args.DATASET = 'Kandinsky'        \n", "        args.content_img_file = 'YellowLabradorLooking_new.jpg'\n", "        args.content_size = (512, 512)        \n", "        args.init_img_name = 'YellowLabradorLooking_new.jpg'\n", "        args.style_imgs_files = ['Vassily_Kandinsky,_1913_-_Composition_7.jpg']\n", "    if 0:\n", "        args.PROJECT = 'lion'\n", "        args.DATASET = 'lion'        \n", "        args.content_img_file = 'lion.jpg'\n", "        args.content_size = (512, 512)        \n", "        args.init_img_name = 'lion.jpg'\n", "        args.style_imgs_files = ['a-hymn-to-the-shulamite-1982.jpg']\n", "    if 1:\n", "        args.PROJECT = 'lion'\n", "        args.DATASET = 'lion'        \n", "        args.content_img_file = 'lion.jpg'\n", "        args.content_size = (512, 512)        \n", "        args.init_img_name = 'lion.jpg'\n", "        args.style_imgs_files = ['starry-night.jpg']\n", "    if 0:\n", "        args.PROJECT = 'madrid'\n", "        args.DATASET = 'madrid'          \n", "        args.content_img_file = 'madrid.JPG'\n", "        args.content_size = (512, 512)\n", "        args.init_img_name = 'madrid.JPG'\n", "        args.style_imgs_files = ['starry-night.jpg']\n\n", "        #args.style_mask_imgs_files = [onimg.name_to_maskname(item) for item in args.style_imgs_files]\n", "        args.style_mask_imgs_files = ['madrid_mask.JPG' for item in args.style_imgs_files]\n", "        #args.style_imgs_thress = [(100,cv2.THRESH_BINARY)]\n", "        #args.style_imgs_thress = [(145,cv2.THRESH_BINARY)]\n", "        #args.style_imgs_thress = [(38,cv2.THRESH_BINARY)]\n", "        args.style_imgs_thress = [(38, cv2.THRESH_BINARY_INV)]\n", "    if 0:\n", "        args.PROJECT = 'portubridge'\n", "        args.DATASET = 'portubridge'        \n", "        args.content_img_file = 'portu_frame0000.jpg'\n", "        args.content_size = (512, 512)        \n", "        args.init_img_name = 'portu_frame0000.jpg'\n", "        args.style_imgs_files = ['kandinsky.jpg']\n", "    args = onutil.pargs(vars(args))\n", "    xp = getxp(vars(args))\n", "    args = onutil.pargs(xp)    \n", "    onutil.ddict(vars(args), 'args')\n", "    if 1: # config\n", "        print(f\"|===> nnimg: config \\n \\\n", "            args.video: {args.video} : content images from frames \\n \\\n", "            args.show_entry_imgs: {args.show_entry_imgs} \\n \\\n", "            args.steps_per_epoch: {args.steps_per_epoch} \\n \\\n", "            args.model_weights: {args.model_weights} \\n \\\n", "            args.max_epochs: {args.max_epochs} \\n \\\n", "        \")\n", "    if 1: # tree\n", "        args.code_dir = os.path.join(args.proto_dir, 'code') # up project dir\n", "        #args.data_dir = os.path.join(args.proto_dir, 'data')\n", "        args.video_input_path = os.path.join(args.dataorg_dir, args.video_file)\n", "        args.video_frames_dir=os.path.join(args.proj_dir, 'frames')\n", "        args.video_styled_dir=os.path.join(args.proj_dir, 'outstyled')\n", "        args.video_output_dir=os.path.join(args.proj_dir, 'outvid')\n", "        args.video_input_dir = args.video_output_dir # frames _e_\n", "        args.style_imgs_weights = normalize(args.style_imgs_weights)\n", "        args.style_imgs_dir = args.data_dir\n", "        style_base = os.path.splitext(args.style_imgs_files[0])[0]\n", "        args.style_layers_weights = normalize(args.style_layers_weights)\n", "        args.content_layer_weights = normalize(args.content_layer_weights)\n", "        args.content_imgs_dir = args.data_dir\n", "        content_base = os.path.splitext(args.content_img_file)[0]\n", "        args.output_folder = content_base + \"_\" + style_base\n", "        args.img_output_dir = os.path.join(args.results_dir, 'cromes', args.output_folder)\n", "        args.init_img_dir = args.data_dir\n", "        print(f\"|===> nnimg: tree \\n \\\n", "            args.output_folder: {args.output_folder} \\n \\\n", "            args.img_output_dir: {args.img_output_dir} \\n \\\n", "            \\n \\\n", "            args.proto_dir: {args.proto_dir} \\n \\\n", "            args.data_dir: {args.data_dir} \\n \\\n", "            args.content_imgs_dir: {args.content_imgs_dir} \\n \\\n", "            args.init_img_dir: {args.init_img_dir} \\n \\\n", "            args.style_imgs_dir: {args.style_imgs_dir} \\n \\\n", "        \")\n", "        os.makedirs(args.data_dir, exist_ok=True)\n", "        os.makedirs(args.video_output_dir, exist_ok=True)\n", "        os.makedirs(args.img_output_dir, exist_ok=True)\n", "        os.makedirs(args.content_imgs_dir, exist_ok=True) # data_dir\n", "    if 1: # content image: (422, 512, 3) <class 'numpy.ndarray'>  [[[ 99 165 160]\n", "        content_img_path = os.path.join(args.content_imgs_dir, args.content_img_file)\n", "        img_file = os.path.basename(content_img_path)\n", "        if not os.path.exists(content_img_path):\n", "            urlfolder = 'https://github.com/xueyangfu/neural-style-tf/raw/master/image_input/'\n", "            url = f'{urlfolder}{img_file}'\n", "            topath = os.path.join(args.content_imgs_dir, f'{img_file}')\n", "            print(f\"|===> nnimg: content file does not exist\\n \\\n", "                content_img_path: {content_img_path} \\n \\\n", "                urlfolder: {urlfolder} \\n \\\n", "                url: {url} \\n \\\n", "                topath: {topath} \\n \\\n", "                args.content_imgs_dir: {args.content_imgs_dir} \\n \\\n", "            \")\n", "            tofile = tf.keras.utils.get_file(f'{topath}', origin=url, extract=True)\n", "        assert os.path.exists(content_img_path), f\"content image {content_img_path} does not exist\"\n", "        img = onfile.path_cv_pil(content_img_path)\n", "        img = ondata.pil_resize(img, ps=args.content_size)\n", "        img = onformat.pil_to_dnua(img)\n", "        print(f\"|===> nnimg: content \\n \\\n", "            content_img_path: {content_img_path} \\n \\\n", "            args.content_imgs_dir: {args.content_imgs_dir} \\n \\\n", "            img shape: {np.shape(img)} \\n \\\n", "        \")\n", "        content_img = img\n", "    if args.visual > 1: # show content image\n", "        print(f'|---> pil content img')\n", "        onplot.pil_show_nua(content_img) # non interrupt\n", "    if args.visual > 1:# show content image from path\n", "        print(f'|---> cv content img')\n", "        content_img_path = os.path.join(args.content_imgs_dir, args.content_img_file)\n", "        onplot.cv_path(content_img_path, size = 512, title='content img', wait=2000)\n", "    if 1: #   input image: (422, 512, 3) <class 'numpy.ndarray'> [[[ 99 165 160]\n", "        init_path = os.path.join(args.init_img_dir, args.init_img_name)\n", "        init_image = onfile.path_to_tnua_with_tf(init_path, args)\n", "        print(f\"|===> nnimg: init \\n \\\n", "            init_path: {init_path} \\n \\\n", "            args.content_imgs_dir: {args.content_imgs_dir} \\n \\\n", "            init_shape: {np.shape(init_image)} \\n \\\n", "        \")\n", "        if args.visual > 1:\n", "            print(f'|---> vis init')\n", "            onplot.pil_show_nua(init_image, \"[   .   ] init_image\")\n", "    if 1: #   style images\n", "        style_imgs_paths = onfile.names_to_paths(args.style_imgs_files, args.style_imgs_dir)\n", "        for path in style_imgs_paths:\n", "            print(f'path: {path}')\n", "            if not os.path.exists(path):\n", "                print(f'path: {path} DOES NOT EXIST')\n", "                urlfolder = 'https://raw.github.com/xueyangfu/neural-style-tf/master/styles/'\n", "                file_name = os.path.basename(path)\n", "                url = f'{urlfolder}{file_name}'\n", "                topath = os.path.join(args.style_imgs_dir, file_name)\n", "                tofile = tf.keras.utils.get_file(f'{topath}', origin=url, extract=True)\n", "        style_imgs = onfile.names_to_nuas_with_tf(args.style_imgs_files, args.style_imgs_dir, args)\n", "        print(f\"|===> nnimg: styles \\n \\\n", "            args.style_imgs_files: {args.style_imgs_files} \\n \\\n", "            args.style_imgs_dir: {args.style_imgs_dir} \\n \\\n", "            shapes: {[str(np.shape(style_imgs[i])) for i,img in enumerate(style_imgs)]} \\n \\\n", "        \")\n", "        if args.visual > 1:\n", "            print(f'|---> vis styles')\n", "            onplot.pil_show_nuas(style_imgs, [\"[   .   ] style_imgs\"])\n", "    if 1: #   mask style images\n", "        b,h,w,c = np.shape(content_img)\n", "        csize = (w,h)\n", "        print(f'|===> nnimg: mask style images \\n \\\n", "            masks are regions of the content(input) image impacted by style transfer \\n \\\n", "            content image size: {csize} \\n \\\n", "        ')\n", "        args.style_mask = 0\n", "        if 'style_mask_imgs_files' in vars(args).keys():\n", "            if args.style_mask_imgs_files:\n", "                for maskfile, thress in zip(args.style_mask_imgs_files, args.style_imgs_thress):\n", "                    maskpath = os.path.join(args.style_imgs_dir, maskfile)\n", "                    if os.path.exists(maskpath):\n", "                        print(f'|... nnimg: mask file exists - assume ok, delete if not \\n \\\n", "                            maskpath: {maskpath} \\n \\\n", "                            maskfile: {maskfile} \\n \\\n", "                            thress: {thress} \\n \\\n", "                        ')\n", "                        args.style_mask = 1\n", "                    else:\n", "   \n", "                        unmaskfile = onimg.name_to_unmaskname(maskfile)\n", "                        unmaskpath =  os.path.join(args.style_imgs_dir, unmaskfile)\n", "                        print(f'|... nnimg: maskpath {maskpath} does NOT exist \\n \\\n", "                            generate {maskpath} from {unmaskpath} \\n \\\n", "                            with thress {thress} \\n \\\n", "                            the mask has to be created from {unmaskpath} with same size than imput img: {csize} \\n \\\n", "                        ')\n", "    \n", "                        style_img_to_mask = onfile.path_to_cvi(unmaskpath)\n", "                        style_img_to_mask = cv2.resize(style_img_to_mask, dsize=csize, interpolation=cv2.INTER_AREA)\n", "                        if args.visual > 1:\n", "                            cv2.imshow('style_img_to_mask', style_img_to_mask)             \n", "                            cv2.waitKey(0) & 0xFF is 27\n", "                            cv2.destroyAllWindows()\n", "                        style_mask = onimg.img_to_mask(\n", "                                style_img_to_mask, outpath=maskpath,\n", "                                height=None,width=None,\n", "                                threshold=thress[0], # 77,\n", "                                thresmode=thress[1], #cv2.THRESH_BINARY,\n", "                                visual=1, verbose=1)                        \n", "                        args.style_mask = 1\n", "            args.style_mask = 1\n", "            args.style_mask_imgs = args.style_mask_imgs_files\n", "        if args.style_mask: # _e_\n", "            style_mask_imgs = onfile.names_to_nuas_with_tf(args.style_mask_imgs_files, args.style_imgs_dir, args,)\n", "            print(f\"|===> nnimg: masks \\n \\\n", "                args.style_mask_imgs_files: {args.style_mask_imgs_files} \\n \\\n", "                args.style_imgs_dir: {args.style_imgs_dir} \\n \\\n", "                shapes: {[str(np.shape(style_mask_imgs[i])) for i,img in enumerate(style_mask_imgs)]} \\n \\\n", "            \")\n", "            if args.visual > 0:\n", "                print(f'|---> vis masks')\n", "                onplot.pil_show_nuas(style_mask_imgs, [\"[   .   ] style_mask_imgs\"])\n", "    if 0: #   mask content images\n", "        print(\"|===> mask content images\")\n", "        \n", "        name_base = os.path.splitext(args.content_img_file)[0]\n", "        name_ext = os.path.splitext(args.content_img_file)[1]\n", "        print(\"|...> name_base\", name_base, name_ext)\n", "        name_mask = f'{name_base}_mask{name_ext}'\n", "        print(\"|...> name_mask\", name_mask)\n", "        (b,h,w,c) = np.shape(content_img)\n", "        dim = (w, h)\n", "        print(\"|...> content_size\", dim)\n", "        outpath = os.path.join(args.data_dir, name_mask)\n", "        img = onformat.nua_to_pil(content_img)\n", "        cvi = onformat.pil_to_cvi(img)\n", "        content_img_mask = onimg.img_to_mask(\n", "            cvi,outpath=outpath,\n", "            height=h,width=w,\n", "            threshold=38,\n", "            thresmode=cv2.THRESH_BINARY\n", "        )\n", "        cvimasked = onimg.cvi_mask_to_cvi(cvi, content_img_mask, op=4)\n", "        if args.visual > 1:\n", "            cv2.imshow('content img', cvi)                 \n", "            cv2.imshow('content mask', content_img_mask)            \n", "            cv2.imshow('masked img', cvimasked)            \n", "            cv2.waitKey(0) & 0xFF is 27\n", "            cv2.destroyAllWindows()\n", "    if 1: # input image\n", "        print(f'|===> input shape  \\n \\\n", "            cwd: {os.getcwd()} \\n \\\n", "            args.video: {args.video} \\n \\\n", "            args.init_image_type: {args.init_image_type} \\n \\\n", "        ')          \n", "        input_img = get_input_image( # (1, 512, 512, 3)\n", "            args.init_image_type, \n", "            content_img, \n", "            style_imgs,\n", "            init_image,\n", "            args.frame, \n", "            args.video_input_dir, \n", "            args.video_output_dir, \n", "            args.frame_content_frmt,\n", "        )\n", "        input_img = onformat.tnua_resize(input_img, args.max_size, args.max_size)\n", "        if 0 and args.visual:\n", "            onplot.pil_show_nua(input_img, \"[   .   ] input_img\")\n", "    if 1: # model\n", "        b,w,h,c = np.shape(input_img)\n", "        input_shape = (w,h,c)\n", "        print(f'|===> model \\n \\\n", "            input_shape: {input_shape} \\n \\\n", "        ')\n", "        model = GAN( input_shape = input_shape, args = args, )\n", "    if 1: # fit\n", "        print(f'|===> fit input image \\n \\\n", "            cwd: {os.getcwd()} \\n \\\n", "            input_img shape: {np.shape(input_img)} \\n \\\n", "            args.style_mask: {args.style_mask} \\n \\\n", "        ')\n", "        model.fit(\n", "            input_img, content_img, style_imgs, \n", "            frame= None,\n", "            args = args\n", "        )\n", "    if 0: # mask result\n", "        epoch = 10\n", "        basename = f'output_{int(epoch-1)}.png'\n", "        img_output_path = os.path.join(args.img_output_dir, basename)\n", "        print(f'|---> cv img_output_path img {img_output_path}')\n", "        output = onfile.path_to_cvi(img_output_path)\n", "        content_img_path = os.path.join(args.content_imgs_dir, args.content_img_file)\n", "        content = onfile.path_to_cvi(content_img_path)\n", "        content = onplot.cv_resize(content, dim=args.content_size)\n", "        result1 = onimg.cvi_mask_to_cvi(content, content_img_mask, op=1)\n", "        result2 = onimg.cvi_mask_to_cvi(output, content_img_mask, op=-1)\n", "        #result = cv2.addWeighted(result1,0.0,result2,0.0,0)\n", "        result = cv2.add(result1,result2)\n", "        if args.visual > 1:\n", "            cv2.imshow('result1 img', result1)                 \n", "            cv2.imshow('result2 img', result2)                 \n", "            cv2.imshow('result img', result)                 \n", "            cv2.waitKey(0) & 0xFF is 27\n", "            cv2.destroyAllWindows()\n", "#\n", "#\n", "#   nnani\n", "#\n", "def nnani(args, kwargs):\n", "    if 0:\n", "        args = onutil.pargs(vars(args))\n", "        args.PROJECT = 'newyork'\n", "        args.DATASET = 'stransfer'\n", "        args.content_size = (512, 512)   \n", "        args.style_imgs_files = ['starry-night.jpg']        \n", "        args.video_file = 'Streets of New York City 4K video-vCdBIRtsL6o.f313.mp4'\n", "    if 1:\n", "        args = onutil.pargs(vars(args))\n", "        args.PROJECT = 'portu'\n", "        args.DATASET = 'stransfer'\n", "        args.content_size = (512, 512)   \n", "        args.style_imgs_files = ['kandinsky.jpg']        \n", "        args.video_file = 'portu.mp4'    \n", "    args = onutil.pargs(vars(args))\n", "    xp = getxp(vars(args))\n", "    args = onutil.pargs(xp)    \n", "    onutil.ddict(vars(args), 'args')\n", "    if 1: # config\n", "        args.frame_start = 0\n", "        args.frame_end = -1 # num_frames\n", "        args.video = True \n", "    print(f\"|===> nnani config \\n \\\n", "        args.video: {args.video} \\n \\\n", "        args.visual: {args.visual} \\n \\\n", "        args.frame_first_iterations: {args.frame_first_iterations} \\n \\\n", "        args.frame_start: {args.frame_start} \\n \\\n", "        args.max_iterations: {args.max_iterations} \\n \\\n", "        args.style_imgs_files = {args.style_imgs_files} \\n \\\n", "    \")\n", "    if 1: # tree\n", "        extension=os.path.splitext(os.path.basename(args.video_file))[1]\n", "        args.code_dir = os.path.join(args.proto_dir, 'code') # up project dir\n", "        args.video_input_path = os.path.join(args.dataorg_dir, args.video_file)\n", "        args.video_frames_dir=os.path.join(args.proj_dir, 'frames')\n", "        args.video_styled_dir=os.path.join(args.proj_dir, 'outstyled')\n", "        args.video_output_dir=os.path.join(args.proj_dir, 'outvid')\n", "        args.video_input_dir = args.video_frames_dir # frames _e_\n", "        content_filename=os.path.splitext(os.path.basename(args.video_file))[0]\n", "        content_filename=f\"{content_filename}\" # eg portu\n", "        args.img_output_dir=os.path.join(args.proj_dir, 'outimgs')\n", "        args.data_dir = os.path.join(args.proto_dir, 'data')\n", "        args.content_imgs_dir = args.data_dir\n", "        args.init_img_dir = args.data_dir\n", "        args.style_imgs_dir = args.data_dir\n", "        print(f'|===> nnani tree \\n \\\n", "            cwd: {os.getcwd()} \\n \\\n", "            args.video_file: {args.video_file} \\n \\\n", "            content_filename: {content_filename} \\n \\\n", "            args.proto_dir: {args.proto_dir} \\n \\\n", "            args.code_dir: {args.code_dir} \\n \\\n", "            args.video_input_path (input video): {args.video_input_path} \\n \\\n", "            args.style_imgs_dir (style imgs dir): {args.style_imgs_dir} \\n \\\n", "            args.video_frames_dir (raw frames dir): {args.video_frames_dir} \\n \\\n", "            args.video_input_dir (raw frames dir): {args.video_input_dir} \\n \\\n", "            args.video_styled_dir (styled frames dir - per frame): {args.video_styled_dir} \\n \\\n", "            args.img_output_dir (fit control imgs dir - per epoch): {args.img_output_dir} \\n \\\n", "            args.video_output_dir (styled vids dir): {args.video_output_dir} \\n \\\n", "        ')\n", "        os.makedirs(args.results_dir, exist_ok=True)\n", "        os.makedirs(args.video_frames_dir, exist_ok=True)\n", "        os.makedirs(args.video_styled_dir, exist_ok=True)\n", "        os.makedirs(args.video_output_dir, exist_ok=True)\n", "        os.makedirs(args.img_output_dir, exist_ok=True)\n", "        if not args.video_input_path: # try\n", "            urlfolder = 'https://github.com/sifbuilder/eodoes/blob/master/packages/eodoes-eodo-nnstransfer/datasrc/'\n", "            url = f'{urlfolder}{args.video_file}'\n", "            topath = args.video_input_path # os.path.join(args.dataorg_dir, f'{args.video_file}')\n", "            print(f\"|===> nnani: video src does not exist\\n \\\n", "                urlfolder: {urlfolder} \\n \\\n", "                url: {url} \\n \\\n", "                topath: {topath} \\n \\\n", "            \")\n", "            tofile = tf.keras.utils.get_file(f'{topath}', origin=url, extract=True)\n", "        assert os.path.exists(args.video_input_path), f'input vide {args.video_input_path} does not exist'"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    if 1: # git\n", "        onutil.get_git(args.AUTHOR, args.GITPOD, args.code_dir)\n", "    assert os.path.exists(args.code_dir), \"code_dir not found\"        \n", "    os.chdir(args.code_dir) # _e_ not std\n", "    if 1: # vid => frames\n", "        if 1:\n", "            print(f'|===> nnani vid to frames \\n \\\n", "                args.video_input_path: {args.video_input_path} \\n \\\n", "                args.video_frames_dir: {args.video_frames_dir} \\n \\\n", "            ')\n", "            onvid.vid_to_frames(args.video_input_path, args.video_frames_dir, target = 1)\n", "        else:\n", "            frame_pattern = 'frame%04d.ppm'\n", "            cmd = f'ffmpeg -v quiet -i \"{args.video_input_path}\" \"{args.video_frames_dir}/{frame_pattern}\"'\n", "            print(f\"cmd: {cmd}\")\n", "            os.system(cmd)            \n", "    if 0: # frames size\n", "        ppm = os.path.join(args.video_frames_dir, 'frame0000.jpg')\n", "        im = Image.open(ppm)\n", "        width, height = im.size   \n", "        max_size = max(width, height)\n", "        print(f\"|===> nnani frame size \\n \\\n", "            max_size: {max_size} \\n \\\n", "        \")\n", "    if 1: # video\n", "        num_frames = len([name for name in os.listdir(args.video_frames_dir) if os.path.isfile(os.path.join(args.video_frames_dir, name))])\n", "        args.frame_end = num_frames\n", "        found_q_frames = len([name for name in os.listdir(args.video_styled_dir) if os.path.isfile(os.path.join(args.video_styled_dir, name))])\n", "        frame_start = found_q_frames + 1\n", "    \n", "        frame = args.frame_start\n", "        args.frame_name = args.frame_content_frmt.format(str(frame).zfill(args.zfill))\n", "        args.frame_img_path = os.path.join(args.video_frames_dir, args.frame_name) \n", "    print(f\"|===> nnani video \\n \\\n", "        cwd: {os.getcwd()} \\n \\\n", "        num_frames: {num_frames} \\n \\\n", "        args.video_input_path: {args.video_input_path} \\n \\\n", "        args.img_output_dir: {args.img_output_dir} \\n \\\n", "        content_filename: {content_filename} \\n \\\n", "        extension: {extension} \\n \\\n", "        args.style_imgs_dir: {args.style_imgs_dir} \\n \\\n", "        args.style_imgs: {args.style_imgs_files} \\n \\\n", "        args.video_file: {args.video_file} \\n \\\n", "        args.video: {args.video} \\n \\\n", "        args.frame_end: {args.frame_end} \\n \\\n", "        args.input_shape: {args.input_shape} \\n \\\n", "        args.frame_content_frmt: {args.frame_content_frmt} \\n \\\n", "        frame: {frame} \\n \\\n", "        args.frame_name: {args.frame_name} \\n \\\n", "        args.frame_img_path: {args.frame_img_path} \\n \\\n", "    \")\n", "    if 1: # content\n", "        print(f'|===> get content  \\n \\\n", "            cwd: {os.getcwd()} \\n \\\n", "            content_frame: {args.frame_img_path} \\n \\\n", "        ')                \n", "        img = onfile.path_cv_pil(args.frame_img_path)\n", "        img = ondata.pil_resize(img, ps=args.content_size)\n", "        img = onformat.pil_to_dnua(img)\n", "        print(f'|===> nnani: content \\n \\\n", "            args.frame_img_path: {args.frame_img_path} \\n \\\n", "            args.content_imgs_dir: {args.content_imgs_dir} \\n \\\n", "            img shape: {np.shape(img)} \\n \\\n", "        ')\n", "        content_frame = img\n", "        if args.visual > 1:\n", "            onplot.pil_show_nua(content_frame)\n", "    if 1: #   style images\n", "        style_img_paths = [os.path.join(args.style_imgs_dir, item) for item in args.style_imgs_files]\n", "        for style_img_path in style_img_paths:\n", "            print(f'|... style_img_path {style_img_path}')\n", "            if not os.path.exists(style_img_path):\n", "                base_path = style_img_path\n", "                base_file = os.path.basename(style_img_path)\n", "                urlfolder = 'https://raw.githubusercontent.com/sifbuilder/eodoes/master/packages/eodoes-eodo-eonart/img/'\n", "                url = f'{urlfolder}{base_file}'\n", "                print(f\"|===> nnimg: get style base file \\n \\\n", "                    urlfolder: {urlfolder} \\n \\\n", "                    url: {url} \\n \\\n", "                    base_path: {base_path} \\n \\\n", "                \")\n", "                tofile = tf.keras.utils.get_file(f'{base_path}', origin=url, extract=True)\n", "            else:\n", "                base_file = os.path.basename(style_img_path)\n", "                print(f\"|===> style file {base_file} already in {args.style_imgs_dir}\")        \n", "        style_imgs = onfile.names_to_nuas_with_tf(args.style_imgs_files, args.style_imgs_dir, args,)\n", "        print(f'|===> nnani: styles \\n \\\n", "            args.style_imgs_files: {args.style_imgs_files} \\n \\\n", "            args.style_imgs_dir: {args.style_imgs_dir} \\n \\\n", "            shapes: {[str(np.shape(style_imgs[i])) for i,img in enumerate(style_imgs)]} \\n \\\n", "        ')\n", "        if 0 and args.visual:\n", "            print(f'|---> vis styles')\n", "            onplot.pil_show_nuas(style_imgs, ['[   .   ] style_imgs'])\n", "    if 1: # input shape\n", "        print(f'|===> input shape  \\n \\\n", "            cwd: {os.getcwd()} \\n \\\n", "            args.video: {args.video} \\n \\\n", "            args.frame_first_type: {args.frame_first_type} \\n \\\n", "            frame: {frame} \\n \\\n", "        ')                    \n", "        input_img = get_input_image(\n", "            args.frame_first_type, \n", "            content_frame, \n", "            style_imgs, \n", "            init_img=None, \n", "            frame=frame,  \n", "            args=args\n", "        )\n", "    print(f'|===> nnani video config \\n \\\n", "        cwd: {os.getcwd()} \\n \\\n", "        content_filename: {content_filename} \\n \\\n", "        args.frame_content_frmt: {args.frame_content_frmt} \\n \\\n", "        content_frame shape: {np.shape(content_frame)} \\n \\\n", "        args.style_imgs: ({len(args.style_imgs_files)}) {args.style_imgs_files} \\n \\\n", "        style_imgs shapes: {[np.shape(img) for img in style_imgs]} \\n \\\n", "        args.video_file: {args.video_file} \\n \\\n", "        args.video_output_dir: {args.video_output_dir} \\n \\\n", "        args.video_frames_dir: {args.video_frames_dir} \\n \\\n", "        args.frame_start/frame_end: ({num_frames}) {args.frame_start} : {args.frame_end} \\n \\\n", "        args.input_img shape: {np.shape(input_img)} \\n \\\n", "    ')\n", "    if 1: # model\n", "        b,w,h,c = np.shape(input_img)\n", "        input_shape = (w,h,c)   \n", "        print(f\"|===> nnani model \\n \\\n", "            cwd: {os.getcwd()} \\n \\\n", "            input_shape: {input_shape} \\n \\\n", "            args.video_input_path: {args.video_input_path} \\n \\\n", "            args.img_output_dir: {args.img_output_dir} \\n \\\n", "        \")\n", "        model = GAN(input_shape = input_shape, args = args,)\n", "    if 1: # fit\n", "        print(f'|===> nnani fit \\n \\\n", "            cwd: {os.getcwd()} \\n \\\n", "            args.video_styled_dir: {args.video_styled_dir} \\n \\\n", "            args.img_output_dir: {args.img_output_dir} \\n \\\n", "        ')\n", "        args.max_iterations = args.frame_iterations\n", "        for frame in range(args.frame_start, args.frame_end+1):\n", "            print(f'|...> RENDERING VIDEO FRAME ({args.frame_first_type}): {frame}/{args.frame_end} ----\\n')\n", "            if frame == args.frame_start:\n", "                print(f\"|...> frame_start input_img type: {type(input_img)}\")\n", "                input_img = get_input_image(\n", "                    init_type=args.frame_first_type, \n", "                    content_img=content_frame, \n", "                    style_imgs=style_imgs, \n", "                    init_img=None, \n", "                    frame=frame,  \n", "                    args=args\n", "                )\n", "            else:\n", "                print(f\"|...> other_frame input_img type: {type(input_img)}\")\n", "                input_img = get_input_image(  # (1, 1080, 1440, 3) <class 'numpy.ndarray'>\n", "                    init_type=args.frame_init_type, #\n", "                    content_img=content_frame, \n", "                    style_imgs=style_imgs, \n", "                    init_img=None, \n", "                    frame=frame,  \n", "                    args=args\n", "                )\n", "            input_img = onimg.tf_resize_nua(input_img, args=args)\n", "            print(f'|...> fit input image \\n \\\n", "                cwd: {os.getcwd()} \\n \\\n", "                input_img shape: {np.shape(input_img)} \\n \\\n", "            ')\n", "            model.fit( \n", "                input_img, content_frame, style_imgs,\n", "                frame = frame,                \n", "                args = args,\n", "            )\n", "    if 1: # render stylized video\n", "        \n", "        os.chdir(args.code_dir) # _e_ not std\n", "        print(f'|===> nnani gen gif stylized video \\n \\\n", "            cwd: {os.getcwd()} \\n \\\n", "            video: {args.video} \\n \\\n", "        ')\n", "        cmd = f'|...> cmd: python neural_style.py --video \\\n", "        --video_input_dir \"{args.video_input_dir}\" \\\n", "        --style_imgs_dir \"{args.style_imgs_dir}\" \\\n", "        --style_imgs {args.style_imgs_files[0]} \\\n", "        --frame_end {args.frame_end} \\\n", "        --max_size {args.max_size} \\\n", "        --verbose'\n", "        print(cmd)\n", "        os.system(cmd)\n", "    if 1: # gen gif video\n", "        fps = 6\n", "        maxTime = 9 # seconds\n", "        frameCount = 0\n", "        time = 0\n", "        nframes = int( maxTime*fps )\n", "        qsegs = 7\n", "        qcells = qsegs * qsegs\n", "        fps=10\n", "        gif_output_path = os.path.join(args.video_output_dir, 'v.gif')\n", "        outvidframes_dir = args.img_output_dir # args.video_frames_dir\n", "        print(f'|===> nnani gen gif stylized video \\n \\\n", "            from args.video_styled_dir: {args.video_styled_dir} \\n \\\n", "            to video_output_path: {gif_output_path} \\n \\\n", "        ')\n", "        onvid.folder_to_gif(args.video_styled_dir, gif_output_path)\n", "#                \n", "    if 1: # gen mp4 video\n", "        fps = 6\n", "        maxTime = 9 # seconds\n", "        frameCount = 0\n", "        time = 0\n", "        nframes = int( maxTime*fps )\n", "        qsegs = 7\n", "        qcells = qsegs * qsegs\n", "        fps=10\n", "        video_output_path = os.path.join(args.video_output_dir, 'v.mp4')\n", "        outvidframes_dir = args.img_output_dir # args.video_frames_dir\n", "        print(f'|===> nnani gen mp4 stylized video \\n \\\n", "            from args.video_frames_dir: {args.video_styled_dir} \\n \\\n", "            to video_output_path: {video_output_path} \\n \\\n", "        ')\n", "        onvid.frames_to_video(args.video_styled_dir, video_output_path, fps)\n", "#\n", "#\n", "#\n", "#   MAIN\n", "#\n", "#\n", "def main():\n", "    parser = argparse.ArgumentParser(description='Run \"python %(prog)s <subcommand> --help\" for subcommand help.')\n", "    onutil.dodrive()\n", "    ap = getap()\n", "    for p in ap:\n", "        cls = type(ap[p])\n", "        parser.add_argument('--'+p, type=cls, default=ap[p])\n", "    cmds = [key for key in globals() if key.startswith(\"nn\")]\n", "    primecmd = ap[\"primecmd\"]\n", "        \n", "    # ---------------------------------------------------------------\n", "    #   add subparsers\n", "    #\n", "    subparsers = parser.add_subparsers(help='subcommands', dest='command') # command - subparser\n", "    for cmd in cmds:\n", "        subparser = subparsers.add_parser(cmd, help='cmd')  # add subcommands\n", "    \n", "    subparsers_actions = [action for action in parser._actions\n", "        if isinstance(action, argparse._SubParsersAction)] # retrieve subparsers from parser\n", "  \n", "    for subparsers_action in subparsers_actions:  # add common       \n", "        for choice, subparser in subparsers_action.choices.items(): # get all subparsers and print help\n", "            for p in {}:  # subcommand args dict\n", "                cls = type(ap[p])\n", "                subparser.add_argument('--'+p, type=cls, default=ap[p])\n\n", "    # get args to pass to nn cmds\n", "    if onutil.incolab():\n", "        args = parser.parse_args('') #  defaults as args\n", "    else:\n", "        args = parser.parse_args() #  parse_arguments()\n", "    kwargs = vars(args)\n", "    subcmd = kwargs.pop('command')      \n", "    if subcmd is None:\n", "        print (f\"Missing subcommand. set to default {primecmd}\")\n", "        subcmd = primecmd\n", "    \n", "    for name in cmds:\n", "        if (subcmd == name):\n", "            print(f'|===> call {name}')\n", "            globals()[name](args, kwargs) # pass args to nn cmd\n", "#\n", "#\n", "#\n", "# python base/base.py nninfo\n", "if __name__ == \"__main__\":\n", "    print(\"|===>\", __name__)\n", "    main()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}